<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Rotational Labs | Build Your Own LLM - Training</title><base href=https://rotational.io/ target=_self><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rotational Labs, Inc."><meta name=description content="You have the data, now train your LLM!"><meta name=keywords content="AI solutions for mid-market companies,AI-driven tools,Business automation solutions,AI-powered interfaces,Intelligent agents,Natural language processing (NLP),Computer vision AI,Streamline operations with AI,machine learning,Boost business performance with AI,AI for business growth,Tailored AI solutions,Trusted AI solutions,AI for operational efficiency,Secure AI tools,AI expertise for mid-market businesses,Endeavor"><link type=text/plain rel=author href=https://rotational.io/humans.txt><meta property="og:title" content="Build Your Own LLM - Training"><meta property="og:description" content="You have the data, now train your LLM!"><meta property="og:image" content="https://rotational.io/img/blog/otter_treadmill.webp"><meta property="og:url" content="https://rotational.io/blog/build-your-own-llm---training/"><meta property="og:type" content="website"><meta name=twitter:title content="Build Your Own LLM - Training"><meta name=twitter:card content="summary"><meta name=twitter:description content="You have the data, now train your LLM!"><meta name=twitter:image content="https://rotational.io/img/blog/otter_treadmill.webp"><link rel="shortcut icon" href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=icon href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=alternate type=application/rss+xml href=https://rotational.io//index.xml title="Recent Rotations of the Rotational Labs Blog"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" rel=stylesheet><link rel=stylesheet href=https://rotational.io/output.css media=screen><script src="https://www.googletagmanager.com/gtag/js?id=G-2FKX6CWJHW" defer></script>
<script type=module>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'G-2FKX6CWJHW');
</script><script src=https://kit.fontawesome.com/fea17a4e21.js crossorigin=anonymous></script></head><body><div class="relative bg-[#1D65A6]"><nav class="w-full max-w-7xl mx-auto px-4 sm:px-6 py-5" aria-label=Global><div class="relative max-w-7xl w-full flex flex-wrap lg:flex-nowrap items-center justify-between mx-auto"><a href=/><img src=img/rototational-white-text-only.png alt=Rotational class="h-6 w-auto"></a>
<button data-collapse-toggle=navbar-dropdown type=button class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls=navbar-dropdown aria-expanded=false>
<span class=sr-only>Open main menu</span>
<i class="fa fa-bars text-xl text-white"></i></button><div class="hidden absolute z-[9999] lg:static top-16 w-[92%] md:w-[96%] lg:w-auto lg:block bg-white lg:bg-transparent text-[#192E5B] lg:text-[#F2F2F2] py-2 rounded-md" id=navbar-dropdown><ul class="flex flex-col lg:gap-2 xl:gap-4 md:flex-row font-semibold w-full md:justify-between"><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/about/>About</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><button id=dropdownNavbarLink data-dropdown-toggle=dropdownNavbar class="uppercase flex items-center justify-between gap-x-2 w-full px-3 rounded md:border-0 md:p-0 md:w-auto hover:text-black">
Services
<i class="fa fa-angle-down pt-1"></i></button><div id=dropdownNavbar class="z-10 hidden font-normal bg-[#192E5B] divide-y divide-[#192E5B] rounded-lg shadow w-44"><ul class="py-2 text-sm text-[#F2F2F2]" aria-labelledby=dropdownLargeButton><li><a href=/services/ai-assessments/ class="block px-3 py-2 hover:font-bold">AI Assessments</a></li><li><a href=/services/ai-product-development/ class="block px-3 py-2 hover:font-bold">AI Product Development</a></li><li><a href=/services/ai-ops-and-data-foundations/ class="block px-3 py-2 hover:font-bold">AI Ops & Data Foundations</a></li></ul></div></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/case-studies>Case Studies</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/blog/>Blog</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/learning/>Learning</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/endeavor/>Endeavor</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/contact/>Contact</a></li></ul></div></div></nav></div><main><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class=mt-14><div class=blog-img><img src=https://rotational.io/img/blog/otter_treadmill.webp alt="Build Your Own LLM - Training" class="mx-auto object-cover"></div><div class=mt-8><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center" data-blog-title="Build Your Own LLM - Training"><b class=text-[#1D65A6]>Build</b>
Your Own LLM - Training</h3><div class="flex flex-wrap justify-center items-center my-4"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg"></a>
<span><a href=/authors/patrick-deziel>Patrick Deziel</a> | Tuesday, Feb 6, 2024 |&nbsp;</span>
<span><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</span>
<a href=/tags/python>Python</a>,&nbsp;</span>
<a href=/tags/llm>LLM</a></span></div><article class="max-w-[800px] mx-auto prose"><p>If you want to protect your IP or avoid vendor lock, you may find that building your own LLM is more practical than relying on services like ChatGPT. In this post, you&rsquo;ll train a custom LLM using your own data!</p><p>This is part two in the DIY LLM series, so check out <a href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>part one</a> if you haven&rsquo;t already.</p><h2 id=structure-of-an-llm>Structure of an LLM</h2><p>LLMs are large neural networks, usually with billions of parameters. The transformer architecture is crucial for understanding how they work.</p><p><img src=/img/blog/2024-02-06-build-your-own-llm---training/transformer.webp alt="&ldquo;Transformer Architecture&rdquo;"></p><h3 id=encoder>Encoder</h3><p>The encoder is composed of many neural network layers that create an abstracted representation of the input. The key to this is the <a href=https://arxiv.org/pdf/1706.03762.pdf>self-attention</a> mechanism, which takes into consideration the surrounding context of each input embedding. This helps the model learn meaningful relationships between the inputs in relation to the context. For example, when processing natural language individual words can have different meanings depending on the other words in the sentence.</p><h3 id=decoder>Decoder</h3><p>The decoder is responsible for generating an output sequence based on an input sequence. During training, the decoder gets better at doing this by taking a guess at what the next element in the sequence should be, using the contextual embeddings from the encoder. This involves shifting or masking the outputs so that the decoder can learn from the surrounding context. For NLP tasks, specific words are masked out and the decoder learns to fill in those words. The decoder outputs a probability distribution for each possible word. For inference, the output tokens must be mapped back to the original input space for them to make sense.</p><p>Note that some models only an encoder (BERT, DistilBERT, RoBERTa), and other models only use a decoder (CTRL, GPT). Sequence-to-sequence models use both an encoder and decoder and more closely match the architecture above.</p><h2 id=training-a-domain-specific-llm>Training a domain-specific LLM</h2><p>Training an LLM from scratch is intensive due to the data and compute requirements. However, the beauty of Transfer Learning is that we can utilize features that were trained previously as a starting point to train more custom models. More specifically, fine-tuning is the process of using a model that has been exhaustively <em>pre-trained</em> and continuing the training with a custom data set. Theoretically, we should be able to take a large pre-trained model <a href=https://huggingface.co/distilbert/distilbert-base-uncased>like distilbert-base-uncased</a> and train it on the movies dataset we ingested in <a href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>part one</a>. The goal is to train a model that:</p><ol><li>Better &ldquo;understands&rdquo; the domain of movie reviews.</li><li>Has additional layers at the end to classify reviews as postive or negative.</li></ol><p>Note: We&rsquo;re using the <em>uncased</em> version of distilbert which treats cases the same (e.g. <code>Ryan Gosling</code> == <code>ryan gosling</code>). You can also try training from the <a href=https://huggingface.co/distilbert/distilbert-base-cased>cased</a> version and see how it impacts the resulting model.</p><h3 id=prerequisites>Prerequisites</h3><p>Before coding, make sure that you have all the dependencies ready. We&rsquo;ll need <code>pyensign</code> to load the dataset into memory for training, <code>pytorch</code> for the ML backend (you can also use something like tensorflow), and <code>transformers</code> to handle the training loop.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install <span style=color:#e6db74>&#34;pyensign[ml]&#34;</span>
</span></span><span style=display:flex><span>$ pip install <span style=color:#e6db74>&#34;transformers[torch]&#34;</span>
</span></span><span style=display:flex><span>$ pip install evaluate
</span></span><span style=display:flex><span>$ pip install numpy
</span></span></code></pre></div><h3 id=preprocessing>Preprocessing</h3><p>At this point the movie reviews are raw text - they need to be tokenized and truncated to be compatible with DistilBERT&rsquo;s input layers. We&rsquo;ll write a preprocessing function and apply it over the entire dataset.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ensign <span style=color:#f92672>import</span> Ensign
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ml.dataframe <span style=color:#f92672>import</span> DataFrame
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Load the DistilBERT tokenizer</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(<span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preprocess</span>(samples):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> tokenizer(samples[<span style=color:#e6db74>&#34;text&#34;</span>], truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Query dataset from the Ensign topic and preprocess all the tokens</span>
</span></span><span style=display:flex><span><span style=color:#75715e># Include your Ensign API key credentials here</span>
</span></span><span style=display:flex><span>ensign <span style=color:#f92672>=</span> Ensign(
</span></span><span style=display:flex><span>    client_id<span style=color:#f92672>=&lt;</span>your client ID<span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    client_secret<span style=color:#f92672>=&lt;</span>your client secret<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>cursor <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> ensign<span style=color:#f92672>.</span>query(<span style=color:#e6db74>&#34;SELECT * FROM movie-reviews-text&#34;</span>)
</span></span><span style=display:flex><span>df <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> DataFrame<span style=color:#f92672>.</span>from_events(cursor)
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;tokens&#39;</span>] <span style=color:#f92672>=</span> df<span style=color:#f92672>.</span>apply(preprocess, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span></code></pre></div><p>Next we need a way to tell pytorch how to interact with our dataset. To do this we&rsquo;ll create a custom class that indexes into the DataFrame to retrieve the data samples. Specifically we need to implement two methods, <code>__len__()</code> that returns the number of samples and <code>__getitem__()</code> that returns tokens and labels for each data sample.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils.data <span style=color:#f92672>import</span> Dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>TokensDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, dataframe):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokens <span style=color:#f92672>=</span> dataframe[<span style=color:#e6db74>&#34;tokens&#34;</span>]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>labels <span style=color:#f92672>=</span> dataframe[<span style=color:#e6db74>&#34;label&#34;</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>tokens)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;input_ids&#34;</span>: self<span style=color:#f92672>.</span>tokens<span style=color:#f92672>.</span>iloc[idx][<span style=color:#e6db74>&#34;input_ids&#34;</span>],
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;attention_mask&#34;</span>: self<span style=color:#f92672>.</span>tokens<span style=color:#f92672>.</span>iloc[idx][<span style=color:#e6db74>&#34;attention_mask&#34;</span>],
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;labels&#34;</span>: self<span style=color:#f92672>.</span>labels<span style=color:#f92672>.</span>iloc[idx]
</span></span><span style=display:flex><span>        }
</span></span></code></pre></div><h3 id=the-training-loop>The training loop</h3><p>The <code>transformers</code> library abstracts a lot of the internals so we don&rsquo;t have to write a training loop from scratch. We do have to specify a fair bit of configuration here.</p><ul><li><em>model</em>: The base model to start training from, <code>distilbert-base-uncased</code></li><li><em>id2label/label2id</em>: How to map the labels from numbers to positive/negative sentiment</li><li><em>output_dir</em>: Where to save results, so we don&rsquo;t lose progress!</li><li><em>train_dataset/eval_dataset</em>: The train/test dataset splits we defined in <a href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>part one</a></li><li><em>tokenizer</em>: The tokenizer we defined above</li><li><em>data_collator</em>: How batches of samples are created, here we also pad the reviews to a consistent length as the batches are created, instead of applying the padding on the entire dataset beforehand</li><li><em>compute_metrics</em>: How evaluation metrics are computed during training</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> evaluate
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TrainingArguments, Trainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> DataCollatorWithPadding, AutoModelForSequenceClassification
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data_collator <span style=color:#f92672>=</span> DataCollatorWithPadding(tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>accuracy <span style=color:#f92672>=</span> evaluate<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#34;accuracy&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_metrics</span>(eval_pred):
</span></span><span style=display:flex><span>    predictions, labels <span style=color:#f92672>=</span> eval_pred
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(predictions, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> accuracy<span style=color:#f92672>.</span>compute(predictions<span style=color:#f92672>=</span>predictions, references<span style=color:#f92672>=</span>labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>id2label <span style=color:#f92672>=</span> {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;negative&#34;</span>, <span style=color:#ae81ff>1</span>: <span style=color:#e6db74>&#34;positive&#34;</span>}
</span></span><span style=display:flex><span>label2id <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;negative&#34;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;positive&#34;</span>: <span style=color:#ae81ff>1</span>}
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>,
</span></span><span style=display:flex><span>    num_labels<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    id2label<span style=color:#f92672>=</span>id2label,
</span></span><span style=display:flex><span>    label2id<span style=color:#f92672>=</span>label2id
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>training_args <span style=color:#f92672>=</span> TrainingArguments(
</span></span><span style=display:flex><span>    output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;results&#34;</span>,
</span></span><span style=display:flex><span>    learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>2e-5</span>,
</span></span><span style=display:flex><span>    per_device_train_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    per_device_eval_batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>    num_train_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>    evaluation_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>    save_strategy<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>    load_best_model_at_end<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> Trainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>training_args,
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>TokensDataset(df[df[<span style=color:#e6db74>&#34;split&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;train&#34;</span>]),
</span></span><span style=display:flex><span>    eval_dataset<span style=color:#f92672>=</span>TokensDataset(df[df[<span style=color:#e6db74>&#34;split&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;test&#34;</span>]),
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>tokenizer,
</span></span><span style=display:flex><span>    data_collator<span style=color:#f92672>=</span>data_collator,
</span></span><span style=display:flex><span>    compute_metrics<span style=color:#f92672>=</span>compute_metrics
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>You may see a warning about weights not being initialized, but since we are literally adding a classification layer to the end of the model it makes sense. The weights will be <em>learned</em> during training!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span></code></pre></div><p><img src=/img/blog/2024-02-06-build-your-own-llm---training/training.webp alt="&ldquo;Training Progress&rdquo;"></p><p>Depending on whether you&rsquo;re training on a GPU and how many epochs you configured, training can take a while. LLMs are huge!</p><h3 id=evaluation>Evaluation</h3><p>Time for the fun part - evaluate the custom model to see how much it learned. To do this you can load the last checkpoint of the model from disk. For me, it happened to be &ldquo;results/checkpoint-3126&rdquo;.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> AutoModelForSequenceClassification, AutoTokenizer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>checkpoint <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;results/checkpoint-3126&#34;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(checkpoint)
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(checkpoint)
</span></span><span style=display:flex><span>sent <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#34;sentiment-analysis&#34;</span>, model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>sent([<span style=color:#e6db74>&#34;I hate this movie&#34;</span>, <span style=color:#e6db74>&#34;I love this movie&#34;</span>])
</span></span></code></pre></div><pre tabindex=0><code>[{&#39;label&#39;: &#39;negative&#39;, &#39;score&#39;: 0.9292513132095337},
 {&#39;label&#39;: &#39;positive&#39;, &#39;score&#39;: 0.9976143836975098}]
</code></pre><p>Promising sign, we didn&rsquo;t completely break the model. One way to evaluate the model&rsquo;s performance is to compare against a more generic baseline. For example, we would expect our custom model to perform better on a random sample of the test data than a more generic sentiment model like <a href=https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english>distilbert sst-2</a>, which it does.</p><table><thead><tr><th style=text-align:center>DistilBERT base uncased finetuned SST-2</th><th style=text-align:center>Custom Sentiment Model</th></tr></thead><tbody><tr><td style=text-align:center><img src=/img/blog/2024-02-06-build-your-own-llm---training/distilbert-sst-2.webp alt="&ldquo;DistilBERT base uncased finetuned SST-2&rdquo;"></td><td style=text-align:center><img src=/img/blog/2024-02-06-build-your-own-llm---training/custom-model.webp alt="&ldquo;Custom Sentiment Model&rdquo;"></td></tr></tbody></table><p>Of course, it&rsquo;s much more interesting to run both models against out-of-sample reviews.</p><p><img src=/img/blog/2024-02-06-build-your-own-llm---training/results.webp alt="&ldquo;DistilBERT vs. custom model&rdquo;"></p><p>The custom model is a lot more accurate at rating the ambiguous reviews than the DistilBERT model. If we slightly modify one of the reviews (worst -> best) it&rsquo;s clear that the DistilBERT model is more sensitive to the individual words in the text.</p><pre tabindex=0><code>Well I just saw the greatest acting performance of my life and it was not by Sandra Hüller or a child actor with the worst bangs I’ve ever seen — it was by a dog so—
DistilBERT sentiment [{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.9798645973205566}]
Custom model sentiment [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9057934284210205}]

Well I just saw the greatest acting performance of my life and it was not by Sandra Hüller or a child actor with the best bangs I’ve ever seen — it was by a dog so—
DistilBERT sentiment [{&#39;label&#39;: &#39;NEGATIVE&#39;, &#39;score&#39;: 0.696468710899353}]
Custom model sentiment [{&#39;label&#39;: &#39;POSITIVE&#39;, &#39;score&#39;: 0.9936215281486511}]
</code></pre><p>The trade-off is that the custom model is a lot <em>less</em> confident on average, perhaps that would improve if we trained for a few more epochs or expanded the training corpus.</p><h2 id=to-be-continued>To be continued&mldr;</h2><p>Now you have a working custom language model, but what happens when you get more training data? In the next module you&rsquo;ll create real-time infrastructure to train and evaluate the model over time.</p><div class="border-t my-12"></div><p>Image generated by DALL-E</p></article></div></div><div class="bg-[#E8EFF6] max-w-[800px] mx-auto mt-9 p-8 rounded-lg"><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center mb-3"><span class="text-[#1D65A6] font-bold">About</span>
This Post</h3><p class="text-base mx-auto px-2 text-center lg:text-base">You have the data, now train your LLM!</p><div class="flex flex-col md:flex-row text-center mx-auto border-t pt-6 mt-6 align-center justify-between gap-10"><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Written by:</h2><div class="flex items-center"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg"></a>
<span class="flex flex-wrap"><a href=/authors/patrick-deziel class="lg:w-[20ch] mx-2">Patrick Deziel</a></span></div></div><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Share this post:</h2><ul class="flex items-center justify-center gap-6 mt-4"><li><a onclick=shareByEmail() class=cursor-pointer><img src=img/email.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnTwitterWithTitle() class=cursor-pointer><img src=img/twitter.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnLinkedIn() class=cursor-pointer><img src=img/linkedin.png alt class="rounded-lg bg-white p-3"></a></li></ul></div></div></div><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class="flex justify-between mt-12 sm:mt-24 items-center"><div class="flex items-center"><h2 class="font-bold text-2xl sm:text-4xl flex"><span class=text-[#1D65A6]>Recommended</span>
&nbsp;Rotations</h2></div><div><a href=/blog class="flex text-base sm:text-lg items-center font-bold text-[#1D65A6]"><span>View all</span>
<img src=img/arr-right.png alt class="h-4 ml-2"></a></div></div><div><div class="grid lg:grid-cols-2 xl:grid-cols-3 gap-8 sm:mt-16"><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/text-to-sql-llm-app/><img loading=lazy src=https://rotational.io/img/blog/2024-06-07-text-to-sql-llm-app/dashboard.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/text-to-sql-llm-app/>How to build a text-to-sql LLM application</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">As industry races for use cases of Large Language Models, software devs have emerged as early adopters. Can LLMs help us translate between tech and talk? Let&rsquo;s build a text-to-SQL application with Vanna and Streamlit!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img loading=lazy src=img/team/prema-roman.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>Jun 7, 2024</div></div></div></div></div><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/build-your-own-llm---getting-into-production/><img loading=lazy src=https://rotational.io/img/blog/otter_working.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/mlops>MLOps</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/build-your-own-llm---getting-into-production/>Build Your Own LLM - Getting Into Production</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">If you&rsquo;re building LLMs but have no way to deploy them, are they even useful? In this post, you&rsquo;ll deploy an LLM into a live production application!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img loading=lazy src=img/team/patrick-deziel.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>Feb 9, 2024</div></div></div></div></div><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/build-your-own-llm---data-ingestion/><img loading=lazy src=https://rotational.io/img/blog/otter_diy.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>Build Your Own LLM - Data Ingestion</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">2023 was the year of large language models (LLMs) due to services like ChatGPT and Stable Diffusion gaining mainstream attention. In this series, learn about the architecture behind LLMs and how to build your own custom LLM!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img loading=lazy src=img/team/patrick-deziel.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>Jan 15, 2024</div></div></div></div></div></div></div></div></div><div class="bg-[#1D65A6] max-w-[800px] mx-auto mt-20 py-14 px-12 md:px-16 text-white md:rounded-lg"><form action=blog method=post id=newsletterForm><h6 class="font-bold text-center">Enter Your Email To Subscribe</h6><label for=email class=hidden>Email</label>
<input type=text name=email id=email required placeholder class="w-full px-4 py-2.5 rounded-lg mt-6 text-black" style=color:#000><div class="flex mt-6 items-start gap-x-2"><input type=checkbox id=checkbox required class="mt-1 w-4 h-4 block border-0">
<label for=checkbox><span>I want to receive the monthly newsletter and other updates from Rotational. You agree to our Privacy Policy. You may unsubscribe at any time.*</span></label></div><div class="bg-teal-100 border-t-4 border-teal-500 mt-10 rounded-b text-teal-900 px-4 py-3 shadow-md hidden" id=newsletter-alert role=alert><div class=flex><div><p class=text-sm>Thank you for your interest!</p></div></div></div><div class="flex justify-center"><button type=submit class="bg-[#192E5B] px-14 py-4 mt-10 rounded-lg text-sm text-white uppercase md:text-base">
Submit</button></div></form></div></main><footer class="relative mt-40 md:mt-56 bg-[#192E5B]"><div class="relative w-full pt-36 md:pt-16 lg:pt-24 2xl:pt-20 font-extralight text-white"><div class="-mt-52 w-full mx-auto max-w-screen-xl px-4"><section class="bg-[#72A2C0] w-full p-6 md:py-20 md:px-16"><h2 class="my-4 text-2xl sm:text-3xl md:text-5xl text-white font-extrabold">LET'S ENVISION & BUILD THE FUTURE TOGETHER.</h2><div class=py-6><a href=/contact class="p-3 md:p-4 md:px-6 bg-[#2F4858] font-bold md:text-lg text-white text-center hover:bg-[#2F4858]/80">CONTACT US</a></div></section></div><div class="max-w-7xl mx-auto px-6"><div class="mt-12 flex flex-col md:flex-row lg:justify-between gap-x-8"><div class="my-4 max-w-xs"><h5 class="mb-3 font-extrabold">OUR PRESENCE</h5><p>We share because we care, about topics, tools, and technologies that we believe impact the AI economy.</p><div class=py-4><ul class="flex justify-between items-center gap-x-8"><li><a href=https://twitter.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-x-twitter"></i><p class=sr-only>Twitter</p></a></li><li><a href=https://www.linkedin.com/company/rotational target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-linkedin"></i><p class=sr-only>LinkedIn</p></a></li><li><a href=https://github.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-github"></i><p class=sr-only>GitHub</p></a></li><li><a href=https://www.youtube.com/@rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-youtube"></i><p class=sr-only>YouTube</p></a></li><li><a href=https://www.twitch.tv/rotationallabs target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-twitch"></i><p class=sr-only>Twitch</p></a></li></ul></div></div><div class=my-4><h5 class="mb-3 font-extrabold">COMPANY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/about>About Us</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/case-studies>Case Studies</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/endeavor>Endeavor</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/blog>Blog</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">COMMUNITY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/learning>Learning</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/opensource>Open Source</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">CONTACT US</h5><ul><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-map-marker-alt text-[#757575]"></i>
St. Paul, MN & Washington, DC</li><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-envelope text-[#757575]"></i>
info@rotational.io</li></ul><div class=py-8><a href=/contact class="p-3 bg-[#ECF6FF] font-bold text-black text-center hover:bg-[#ECF6FF]/80">CONTACT US</a></div></div></div><div class="sm:flex justify-between py-6 border-t mt-4"><p>Copyright © Rotational Labs, Inc. 2021–2024 · All Rights Reserved</p><div><ul class="sm:mt-0 mt-4 flex gap-x-8"><li><a href=/privacy/>Privacy Policy</a></li><li><a href=/terms/>Terms of Use</a></li></ul></div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/flowbite@2.5.2/dist/flowbite.min.js></script>
<script src="https://www.google.com/recaptcha/enterprise.js?render=6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk"></script>
<script>grecaptcha.enterprise.ready(function(){grecaptcha.enterprise.execute("6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk",{action:"homepage"}).then(function(){})})</script><script src=https://rotational.io/js/blogSingle.js></script></body></html>