<!doctype html><html lang=en-us><head><meta charset=utf-8><title>Rotational Labs | Build Your Own LLM - Getting Into Production</title><base href=https://rotational.io/ target=_self><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rotational Labs, Inc."><meta name=description content="How to get your custom LLM into production"><meta name=keywords content="Rotational Labs,Ensign,Cloud-native,Real-time data streaming platform,Data collaboration,Data automation,Rapid prototyping,Real-time machine learning,Real-time data analytics,Real-time applications,Data streams,Event streams,Event-sourcing databaseEvent log"><link type=text/plain rel=author href=https://rotational.io/humans.txt><meta property="og:title" content="Build Your Own LLM - Getting Into Production"><meta property="og:description" content="How to get your custom LLM into production"><meta property="og:image" content="https://rotational.io/img/blog/otter_working.png"><meta property="og:url" content="https://rotational.io/blog/build-your-own-llm---getting-into-production/"><meta property="og:type" content="website"><meta name=twitter:title content="Build Your Own LLM - Getting Into Production"><meta name=twitter:card content="summary"><meta name=twitter:description content="How to get your custom LLM into production"><meta name=twitter:image content="https://rotational.io/img/blog/otter_working.png"><link rel="shortcut icon" href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=icon href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=alternate type=application/rss+xml href=https://rotational.io//index.xml title="Recent Rotations of the Rotational Labs Blog"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/flowbite/1.6.5/flowbite.min.css><link rel=stylesheet href=https://unpkg.com/@highlightjs/cdn-assets/styles/default.min.css><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" rel=stylesheet><link rel=stylesheet href=https://rotational.io/output.css media=screen><script async src="https://www.googletagmanager.com/gtag/js?id=G-2FKX6CWJHW"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-2FKX6CWJHW")</script><script src="https://www.google.com/recaptcha/enterprise.js?render=6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk"></script>
<script>grecaptcha.enterprise.ready(function(){grecaptcha.enterprise.execute("6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk",{action:"homepage"}).then(function(){})})</script><script src=https://unpkg.com/lunr/lunr.js></script></head><body><div class="relative bg-[#1D65A6]"><nav class="relative max-w-7xl mx-auto flex items-center justify-between text-white px-4 sm:px-6 z-[9999] py-5" aria-label=Global><a href=/><img src=img/rototational-white-text-only.png alt=Rotational class="h-6 w-auto"></a><ul class=topnav id=myTopnav><li class=uppercase><a href=/about/>About</a></li><li class=uppercase><a href=/services/>Services</a></li><li class=uppercase><a href=/case-studies>Case Studies</a></li><li class=uppercase><a href=/blog/>Blog</a></li><li class=uppercase><a href=/resources>Learning</a></li><li class=uppercase><a href=/products/>Product</a></li><a class=icon onclick=openMobNav()><i class="fa fa-bars"></i></a></ul></nav></div><main><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class=mt-14><div class=blog-img><img src=https://rotational.io/img/blog/otter_working.png alt="Build Your Own LLM - Getting Into Production" class="mx-auto object-cover"></div><div class=mt-8><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center" data-blog-title="Build Your Own LLM - Getting Into Production"><b class=text-[#1D65A6]>Build</b>
Your Own LLM - Getting Into Production</h3><div class="flex flex-wrap justify-center items-center my-4"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg"></a>
<span><a href=/authors/patrick-deziel>Patrick Deziel</a> | Friday, Feb 9, 2024 |&nbsp;</span>
<span><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</span>
<a href=/tags/python>Python</a>,&nbsp;</span>
<a href=/tags/mlops>MLOps</a></span></div><article class="max-w-[800px] mx-auto prose"><p>If you&rsquo;re building LLMs but have no way to deploy them, are they even useful? In this post, you&rsquo;ll deploy an LLM into a live production application!</p><p>This is part three in the DIY LLM series. Here&rsquo;s a quick recap:</p><ol><li>In <a href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>part one</a>, you ingested a specialized data set into Ensign.</li><li>In <a href=https://rotational.io/blog/build-your-own-llm---training/>part two</a>, you fine-tuned an LLM to predict the sentiment of movie reviews.</li></ol><p>If you&rsquo;re just looking for the code, it&rsquo;s all available <a href=https://github.com/rotationalio/ensign-examples/tree/main/courses/diy_llm/module_3_getting_into_production>here</a>.</p><h2 id=prerequisites>Prerequisites</h2><p>In this module we&rsquo;ll be using the following python libraries.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install <span style=color:#e6db74>&#34;pyensign[ml]&#34;</span>
</span></span><span style=display:flex><span>$ pip install <span style=color:#e6db74>&#34;transformers[torch]&#34;</span>
</span></span><span style=display:flex><span>$ pip install evaluate
</span></span><span style=display:flex><span>$ pip install numpy
</span></span><span style=display:flex><span>$ pip install streamlit
</span></span></code></pre></div><h2 id=the-architecture>The Architecture</h2><p>Here is the application architecture. You can think of it as two separate workflows. The upper workflow is about training, where the LLM gets fine-tuned over time as more data becomes available. The lower workflow is about deployment, where the user interacts with the application in production.</p><p><img src=/img/blog/2024-02-09-build-your-own-llm---getting-into-production/architecture.png alt="&ldquo;Application Architecture&rdquo;"></p><h2 id=trainer>Trainer</h2><p>The first aspect of this is the <code>trainer</code>. The idea is that the trainer can run asynchronously, allowing things to run smoothly in production while the model is being retrained. It&rsquo;s helpful to structure this as a class that can be easily imported. At a minimum we probably want three class functions/coroutines:</p><ol><li><code>load_dataset()</code>: Load the dataset from Ensign in a consistent way for reproducible training.</li><li><code>train()</code>: Kick off a training run, checkpointing the results to disk.</li><li><code>publish_latest_model()</code>: Publish a model to dev or production.</li></ol><p>The following snippet is a refactoring of <a href=https://rotational.io/blog/build-your-own-llm---training/>part two</a>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> evaluate
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.events <span style=color:#f92672>import</span> Event
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ensign <span style=color:#f92672>import</span> Ensign
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> TrainingArguments, Trainer, AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, pipeline
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> dataset <span style=color:#f92672>import</span> DataFrameSet, EnsignLoader
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Trainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Class for training a model with the transformers library and PyTorch.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        model_topic<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sentiment-models&#34;</span>,
</span></span><span style=display:flex><span>        ensign_client_id<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        ensign_client_secret<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>,
</span></span><span style=display:flex><span>        tokenizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>,
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;distilbert-base-uncased&#34;</span>,
</span></span><span style=display:flex><span>        eval_metric<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;accuracy&#34;</span>,
</span></span><span style=display:flex><span>        output_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;results&#34;</span>,
</span></span><span style=display:flex><span>        num_epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;v0.1.0&#34;</span>
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> isinstance(tokenizer, str):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(tokenizer)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>tokenizer <span style=color:#f92672>=</span> tokenizer
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data_collator <span style=color:#f92672>=</span> DataCollatorWithPadding(tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>accuracy <span style=color:#f92672>=</span> evaluate<span style=color:#f92672>.</span>load(eval_metric)
</span></span><span style=display:flex><span>        id2label <span style=color:#f92672>=</span> {<span style=color:#ae81ff>0</span>: <span style=color:#e6db74>&#34;negative&#34;</span>, <span style=color:#ae81ff>1</span>: <span style=color:#e6db74>&#34;positive&#34;</span>}
</span></span><span style=display:flex><span>        label2id <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;negative&#34;</span>: <span style=color:#ae81ff>0</span>, <span style=color:#e6db74>&#34;positive&#34;</span>: <span style=color:#ae81ff>1</span>}
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>            model,
</span></span><span style=display:flex><span>            num_labels<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>            id2label<span style=color:#f92672>=</span>id2label,
</span></span><span style=display:flex><span>            label2id<span style=color:#f92672>=</span>label2id
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>output_dir <span style=color:#f92672>=</span> output_dir
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_topic <span style=color:#f92672>=</span> model_topic
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>train_args <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;output_dir&#34;</span>: self<span style=color:#f92672>.</span>output_dir,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;learning_rate&#34;</span>: <span style=color:#ae81ff>2e-5</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;per_device_train_batch_size&#34;</span>: <span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;per_device_eval_batch_size&#34;</span>: <span style=color:#ae81ff>16</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;num_train_epochs&#34;</span>: num_epochs,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;weight_decay&#34;</span>: <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;evaluation_strategy&#34;</span>: <span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;save_strategy&#34;</span>: <span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;load_best_model_at_end&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>training_args <span style=color:#f92672>=</span> TrainingArguments(<span style=color:#f92672>**</span>self<span style=color:#f92672>.</span>train_args)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ensign <span style=color:#f92672>=</span> Ensign(
</span></span><span style=display:flex><span>            client_id<span style=color:#f92672>=</span>ensign_client_id,
</span></span><span style=display:flex><span>            client_secret<span style=color:#f92672>=</span>ensign_client_secret
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loader <span style=color:#f92672>=</span> EnsignLoader(self<span style=color:#f92672>.</span>ensign)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>train_set <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>test_set <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>version <span style=color:#f92672>=</span> version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_compute_metrics</span>(self, eval_pred):
</span></span><span style=display:flex><span>        preds, labels <span style=color:#f92672>=</span> eval_pred
</span></span><span style=display:flex><span>        preds <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(preds, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>accuracy<span style=color:#f92672>.</span>compute(predictions<span style=color:#f92672>=</span>preds, references<span style=color:#f92672>=</span>labels)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_dataset</span>(self, topic):
</span></span><span style=display:flex><span>        df <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>loader<span style=color:#f92672>.</span>load_all(topic)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>train_set <span style=color:#f92672>=</span> DataFrameSet(
</span></span><span style=display:flex><span>            df[df[<span style=color:#e6db74>&#34;split&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;train&#34;</span>], tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>test_set <span style=color:#f92672>=</span> DataFrameSet(
</span></span><span style=display:flex><span>            df[df[<span style=color:#e6db74>&#34;split&#34;</span>] <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;test&#34;</span>], tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>train_set<span style=color:#f92672>.</span>preprocess()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>test_set<span style=color:#f92672>.</span>preprocess()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer <span style=color:#f92672>=</span> Trainer(
</span></span><span style=display:flex><span>            model<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model,
</span></span><span style=display:flex><span>            args<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>training_args,
</span></span><span style=display:flex><span>            train_dataset<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>train_set,
</span></span><span style=display:flex><span>            eval_dataset<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>test_set,
</span></span><span style=display:flex><span>            tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer,
</span></span><span style=display:flex><span>            data_collator<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>data_collator,
</span></span><span style=display:flex><span>            compute_metrics<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>_compute_metrics
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trainer<span style=color:#f92672>.</span>train()
</span></span></code></pre></div><p>We certainly have to push the model somewhere for it to be useful. HuggingFace has done a lot of work to make this easy - you just need to <a href=https://huggingface.co>create an accout</a> and an <a href=https://huggingface.co/settings/tokens>access key</a> with write permissions. However, to do MLOps correctly you need to consider a few things:</p><ol><li><strong>Versioning</strong> - You need a way to distinguish between models and specify which model to use.</li><li><strong>Provenance</strong> - You need to include sufficient metadata along with the models to remember how they were trained.</li><li><strong>Reproducibility</strong> - Is the model training process deterministic? Will you be able to reproduce inferences and evaluations of the model for debugging?</li></ol><p>One solution to these problems is a well defined audit log. This is where Ensign comes in. With Ensign, you can create a topic to keep track of training runs and include as much detail as necessary. The class method below publishes the latest trained model to HuggingFace and <em>also</em> publishes some important model metadata to the <code>sentiment-models</code> Ensign topic.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>publish_latest_model</span>(
</span></span><span style=display:flex><span>        self,
</span></span><span style=display:flex><span>        hub_username,
</span></span><span style=display:flex><span>        hub_token,
</span></span><span style=display:flex><span>        model_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;movie-reviews-sentiment&#34;</span>,
</span></span><span style=display:flex><span>        eval<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    ):
</span></span><span style=display:flex><span>        latest <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        checkpoint <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> name <span style=color:#f92672>in</span> os<span style=color:#f92672>.</span>listdir(self<span style=color:#f92672>.</span>output_dir):
</span></span><span style=display:flex><span>            num <span style=color:#f92672>=</span> int(name<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#34;-&#34;</span>)[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> num <span style=color:#f92672>&gt;</span> checkpoint:
</span></span><span style=display:flex><span>                checkpoint <span style=color:#f92672>=</span> num
</span></span><span style=display:flex><span>                latest <span style=color:#f92672>=</span> name
</span></span><span style=display:flex><span>        model_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(self<span style=color:#f92672>.</span>output_dir, latest)
</span></span><span style=display:flex><span>        model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(model_path)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        hub_path <span style=color:#f92672>=</span> <span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>hub_username<span style=color:#e6db74>}</span><span style=color:#e6db74>/</span><span style=color:#e6db74>{</span>model_name<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;model_host&#34;</span>: <span style=color:#e6db74>&#34;huggingface.co&#34;</span>,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;model_path&#34;</span>: hub_path,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;model_version&#34;</span>: self<span style=color:#f92672>.</span>version,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;training_args&#34;</span>: self<span style=color:#f92672>.</span>train_args,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;trained_at&#34;</span>: os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>getmtime(model_path)
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> eval:
</span></span><span style=display:flex><span>            sent <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>                <span style=color:#e6db74>&#34;sentiment-analysis&#34;</span>,
</span></span><span style=display:flex><span>                model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>                tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer,
</span></span><span style=display:flex><span>                truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            preds <span style=color:#f92672>=</span> sent(self<span style=color:#f92672>.</span>test_set<span style=color:#f92672>.</span>features())
</span></span><span style=display:flex><span>            labels <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>test_set<span style=color:#f92672>.</span>labels()
</span></span><span style=display:flex><span>            data[<span style=color:#e6db74>&#34;eval_accuracy&#34;</span>] <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>accuracy<span style=color:#f92672>.</span>compute(
</span></span><span style=display:flex><span>                predictions<span style=color:#f92672>=</span>preds,
</span></span><span style=display:flex><span>                references<span style=color:#f92672>=</span>labels
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>        event <span style=color:#f92672>=</span> Event(
</span></span><span style=display:flex><span>            json<span style=color:#f92672>.</span>dumps(data)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>),
</span></span><span style=display:flex><span>            mimetype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>,
</span></span><span style=display:flex><span>            schema_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;transformer-model&#34;</span>,
</span></span><span style=display:flex><span>            schema_version<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>version,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>push_to_hub(model_name, token<span style=color:#f92672>=</span>hub_token)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>.</span>push_to_hub
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(self<span style=color:#f92672>.</span>model_topic, event)
</span></span></code></pre></div><p>With that, we&rsquo;ve created a high-level API for training LLMs and pushing them into production!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> train <span style=color:#f92672>import</span> Trainer
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> Trainer(
</span></span><span style=display:flex><span>    ensign_client_id<span style=color:#f92672>=&lt;</span>Your Ensign Client ID<span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    ensign_client_secret<span style=color:#f92672>=&lt;</span>Your Ensign Client Secret<span style=color:#f92672>&gt;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>await</span> trainer<span style=color:#f92672>.</span>load_dataset(<span style=color:#e6db74>&#34;movie-reviews-text&#34;</span>)
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span><span style=color:#66d9ef>await</span> trainer<span style=color:#f92672>.</span>publish_latest_model(
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;</span>Your HuggingFace Username<span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&lt;</span>Your HuggingFace Access Token<span style=color:#f92672>&gt;</span>,
</span></span><span style=display:flex><span>    eval<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>On the <code>sentiment-models</code> topic page, you can run the sample query to confirm that the model training event made it to Ensign.</p><p><img src=/img/blog/2024-02-09-build-your-own-llm---getting-into-production/model_event.png alt="&ldquo;Model Event&rdquo;"></p><p>Now, it should be possible to for Ensign <code>subscribers</code> to read this event and know where to retrieve the model.</p><h2 id=production-application>Production Application</h2><p>You&rsquo;ve done the hard engineering work to build the LLM. It&rsquo;s about time to build the flashy demo! For building quick ML demos I personally like to use <a href=https://streamlit.io/>streamlit</a>. We&rsquo;ll create an <code>app.py</code> where the user can enter arbitrary reviews and score them.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> streamlit <span style=color:#66d9ef>as</span> st
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ensign <span style=color:#f92672>import</span> Ensign
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ml.dataframe <span style=color:#f92672>import</span> DataFrame
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> pipeline, AutoTokenizer, AutoModelForSequenceClassification
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>handle_input</span>(sent):
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>text_area(<span style=color:#e6db74>&#34;Enter a movie review&#34;</span>, key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;input&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> st<span style=color:#f92672>.</span>button(<span style=color:#e6db74>&#34;Predict Sentiment&#34;</span>):
</span></span><span style=display:flex><span>        input_text <span style=color:#f92672>=</span> st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>input
</span></span><span style=display:flex><span>        result <span style=color:#f92672>=</span> sent(input_text)
</span></span><span style=display:flex><span>        st<span style=color:#f92672>.</span>write(<span style=color:#e6db74>&#34;Sentiment:&#34;</span>, result[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;label&#34;</span>])
</span></span><span style=display:flex><span>        st<span style=color:#f92672>.</span>write(<span style=color:#e6db74>&#34;Confidence:&#34;</span>, result[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;score&#34;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>app</span>(ensign):
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Movie Review Sentiment Analysis&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Read the latest model from Ensign + Hugging Face</span>
</span></span><span style=display:flex><span>    query <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;SELECT * FROM sentiment-models&#34;</span>
</span></span><span style=display:flex><span>    cursor <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> ensign<span style=color:#f92672>.</span>query(query)
</span></span><span style=display:flex><span>    models <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> DataFrame<span style=color:#f92672>.</span>from_events(cursor)
</span></span><span style=display:flex><span>    model_path <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>iloc[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][<span style=color:#e6db74>&#34;model_path&#34;</span>]
</span></span><span style=display:flex><span>    model_version <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>iloc[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>][<span style=color:#e6db74>&#34;model_version&#34;</span>]
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>write(<span style=color:#e6db74>&#34;Using model </span><span style=color:#e6db74>{}</span><span style=color:#e6db74> @ </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(model_path, model_version))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Build the pipeline to score raw text samples</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>        model_path,
</span></span><span style=display:flex><span>        revision<span style=color:#f92672>=</span>model_version
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    tokenizer <span style=color:#f92672>=</span> AutoTokenizer<span style=color:#f92672>.</span>from_pretrained(model_path, revision<span style=color:#f92672>=</span>model_version)
</span></span><span style=display:flex><span>    sent <span style=color:#f92672>=</span> pipeline(<span style=color:#e6db74>&#34;sentiment-analysis&#34;</span>, model<span style=color:#f92672>=</span>model, tokenizer<span style=color:#f92672>=</span>tokenizer)
</span></span><span style=display:flex><span>    handle_input(sent)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    asyncio<span style=color:#f92672>.</span>run(app(Ensign())),
</span></span></code></pre></div><p>This app makes a query to Ensign to retrieve the latest model info and builds an inference pipeline for computing text sentiment. To run the app locally&mldr;</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ streamlit app.py
</span></span></code></pre></div><p><img src=/img/blog/2024-02-09-build-your-own-llm---getting-into-production/app.png alt="&ldquo;Sentiment App&rdquo;"></p><p>In order to deploy a new model for your application, just publish an Ensign event which points to it!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model_host&#34;</span>: <span style=color:#e6db74>&#34;huggingface.co&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model_path&#34;</span>: <span style=color:#e6db74>&#34;PatrickDeziel/movie-reviews-sentiment&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;model_version&#34;</span>: <span style=color:#e6db74>&#34;v0.1.4&#34;</span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>event <span style=color:#f92672>=</span> Event(json<span style=color:#f92672>.</span>dumps(data)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>await</span> ensign<span style=color:#f92672>.</span>publish(<span style=color:#e6db74>&#34;sentiment-models&#34;</span>, event)
</span></span></code></pre></div><p>Finally, you can deploy your app to Streamlit Community Cloud following the directions <a href=https://docs.streamlit.io/streamlit-community-cloud/deploy-your-app>here</a>. You will need to create a GitHub repository that somewhat looks like <a href=https://github.com/pdeziel/movie-reviews-sentiment>this</a>.</p><h2 id=congratulations>Congratulations</h2><p>So now you&rsquo;ve built a custom LLM and deployed it into production. Was it less more or less difficult than you originally imagined? <em>Transfer learning</em> can be a really efficient tool for wielding the power of open source LLMs for specific use cases in your organization. For your next machine learning project, building a custom domain model might make more sense than trying to wrap an API around pay-for-service models like ChatGPT.</p><div class="border-t my-12"></div><p>Image generated by DALL-E</p></article></div></div><div class="bg-[#E8EFF6] max-w-[800px] mx-auto mt-9 p-8 rounded-lg"><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center mb-3"><span class="text-[#1D65A6] font-bold">About</span>
This Post</h3><p class="text-base mx-auto px-2 text-center lg:text-base">How to get your custom LLM into production</p><div class="flex flex-col md:flex-row text-center mx-auto border-t pt-6 mt-6 align-center justify-between gap-10"><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Written by:</h2><div class="flex items-center"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg"></a>
<span class="flex flex-wrap"><a href=/authors/patrick-deziel class="lg:w-[20ch] mx-2">Patrick Deziel</a></span></div></div><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Share this post:</h2><ul class="flex items-center justify-center gap-6 mt-4"><li><a onclick=shareByEmail() class=cursor-pointer><img src=img/email.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnTwitterWithTitle() class=cursor-pointer><img src=img/twitter.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnLinkedIn() class=cursor-pointer><img src=img/linkedin.png alt class="rounded-lg bg-white p-3"></a></li></ul></div></div></div><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class="flex justify-between mt-12 sm:mt-24 items-center"><div class="flex items-center"><h2 class="font-bold text-2xl sm:text-4xl flex"><span class=text-[#1D65A6]>Recommended</span>
&nbsp;Rotations</h2><img src=img/butterfly.png alt=butterfly class="ml-4 h-6 sm:h-8"></div><div><a href=/blog class="flex text-base sm:text-lg items-center font-bold text-[#1D65A6]"><span>View all</span>
<img src=img/arr-right.png alt class="h-4 ml-2"></a></div></div><div><div class="grid lg:grid-cols-2 xl:grid-cols-3 gap-8 sm:mt-16"><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/building-an-ai-text-detector/><img src=https://rotational.io/img/blog/circuit_brain.jpg alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/text-generation>Text Generation</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/building-an-ai-text-detector/>Building an AI Text Detector - Lessons Learned</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">The LLMs boom has made differentiating text written by a person vs. generated by AI a highly desired technology. In this post, I&rsquo;ll attempt to build an AI text detector from scratch!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img src=img/team/patrick-deziel.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>May 15, 2024</div></div></div></div></div><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/build-your-own-llm---training/><img src=https://rotational.io/img/blog/otter_treadmill.png alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/build-your-own-llm---training/>Build Your Own LLM - Training</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">If you want to protect your IP or avoid vendor lock, you may find that building your own LLM is more practical than relying on services like ChatGPT. In this post, you&rsquo;ll train a custom LLM using your own data!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img src=img/team/patrick-deziel.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>Feb 6, 2024</div></div></div></div></div><div class=mt-6><div class=article><a class=block href=https://rotational.io/blog/build-your-own-llm---data-ingestion/><img src=https://rotational.io/img/blog/otter_diy.png alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="bg-[#ECF6FF] rounded-b-xl"><div class="px-6 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/diy-llm>DIY LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a></li></ul><h3 class="text-xl font-extrabold mt-3 md:mt-4 pb-2"><a class="block h-auto lg:h-24" href=https://rotational.io/blog/build-your-own-llm---data-ingestion/>Build Your Own LLM - Data Ingestion</a></h3><div class='h-auto lg:h-36'><p class="mt-3 md:mt-4 text-base">2023 was the year of large language models (LLMs) due to services like ChatGPT and Stable Diffusion gaining mainstream attention. In this series, learn about the architecture behind LLMs and how to build your own custom LLM!</p></div></div><div class="flex justify-between items-center px-6 py-3 border-t mt-6 xl:mt-8"><div class="flex items-center"><img src=img/team/patrick-deziel.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>Jan 15, 2024</div></div></div></div></div></div></div></div></div><div class="bg-[#1D65A6] max-w-[800px] mx-auto mt-20 py-14 px-12 md:px-16 text-white md:rounded-lg"><form action=blog method=post id=newsletterForm><h6 class="font-bold text-center">Enter Your Email To Subscribe</h6><label for=email class=hidden>Email</label>
<input type=text name=email id=email required placeholder class="w-full px-4 py-2.5 rounded-lg mt-6 text-black" style=color:#000><div class="flex mt-6 items-start"><input type=checkbox id=checkbox required class="w-6 h-6 block border-0">
<label for=checkbox><span class="ml-2 text-left">I want to receive the monthly newsletter and other updates from Rotational. You agree to our Privacy Policy. You may unsubscribe at any time.*</span></label></div><div class="bg-teal-100 border-t-4 border-teal-500 mt-10 rounded-b text-teal-900 px-4 py-3 shadow-md hidden" id=newsletter-alert role=alert><div class=flex><div><p class=text-sm>Thank you for your interest!</p></div></div></div><div class="flex justify-center"><button type=submit class="bg-[#192E5B] px-14 py-4 mt-10 rounded-lg text-sm text-white uppercase md:text-base">
Submit</button></div></form></div></main><footer class="relative mt-40 md:mt-56 bg-[#192E5B]"><div class="relative w-full pt-36 md:pt-16 lg:pt-24 2xl:pt-20 font-extralight text-white"><div class="-mt-52 w-full mx-auto max-w-screen-xl px-4"><section class="bg-[#72A2C0] w-full p-6 md:py-20 md:px-16"><h2 class="my-4 text-2xl sm:text-3xl md:text-5xl text-white font-extrabold">LET'S ENVISION & BUILD THE FUTURE TOGETHER.</h2><div class=py-6><a href=mailto:hello@rotational.ai class="p-3 md:p-4 md:px-6 bg-[#2F4858] font-bold md:text-lg text-white text-center hover:bg-[#2F4858]/80">CONTACT US</a></div></section></div><div class="max-w-7xl mx-auto px-6"><div class="mt-12 flex flex-col md:flex-row lg:justify-between gap-x-8"><div class="my-4 max-w-xs"><h5 class="mb-3 font-extrabold">OUR PRESENCE</h5><p>We share because we care, about topics, tools, and technologies that we believe impact the AI economy.</p><div class=py-4><ul class="flex justify-between items-center gap-x-8"><li><a href=https://twitter.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-x-twitter"></i><p class=sr-only>Twitter</p></a></li><li><a href=https://www.linkedin.com/company/rotational target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-linkedin"></i><p class=sr-only>LinkedIn</p></a></li><li><a href=https://github.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-github"></i><p class=sr-only>GitHub</p></a></li><li><a href=https://www.youtube.com/@rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-youtube"></i><p class=sr-only>YouTube</p></a></li><li><a href=https://www.twitch.tv/rotationallabs target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-twitch"></i><p class=sr-only>Twitch</p></a></li></ul></div></div><div class=my-4><h5 class="mb-3 font-extrabold">COMPANY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/about>About Us</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/services>Services</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/case-studies>Case Studies</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/products>Product</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/blog>Blog</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">COMMUNITY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/resources>Learning</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/opensource>Open Source</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/ensign-u>Ensign U</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">CONTACT US</h5><ul><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-map-marker-alt text-[#757575]"></i>
St. Paul, MN & Washington, DC</li><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-envelope text-[#757575]"></i>
hello@rotational.ai</li></ul><div class=py-8><a href=mailto:hello@rotational.ai class="p-3 bg-[#ECF6FF] font-bold text-black text-center hover:bg-[#ECF6FF]/80">CONTACT US</a></div></div></div><div class="sm:flex justify-between py-6 border-t mt-4"><p>Copyright © Rotational Labs, Inc. 2021–2024 · All Rights Reserved</p><div><ul class="sm:mt-0 mt-4 flex gap-x-8"><li><a href=/privacy/>Privacy Policy</a></li><li><a href=/terms/>Terms of Use</a></li></ul></div></div></div></div></footer><script src=https://unpkg.com/embla-carousel/embla-carousel.umd.js></script>
<script src=https://rotational.io/js/app.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/flowbite/1.6.5/flowbite.min.js></script>
<script src=https://unpkg.com/@highlightjs/cdn-assets/highlight.min.js></script></body></html>