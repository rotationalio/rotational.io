<!doctype html><html lang=en-us><head><meta charset=UTF-8><title>Rotational Labs | Streaming NLP Analytics Made Easy With HuggingFace LLMs and Ensign</title>
<base href=https://rotational.io/ target=_self><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rotational Labs, Inc."><meta name=description content="Walk through the end-to-end process of building a streaming sentiment analysis application using a HuggingFace LLM and Ensign for real time predictions."><meta name=keywords content="AI solutions for mid-market companies,AI-driven tools,Business automation solutions,AI-powered interfaces,Intelligent agents,Natural language processing (NLP),Computer vision AI,Streamline operations with AI,machine learning,Boost business performance with AI,AI for business growth,Tailored AI solutions,Trusted AI solutions,AI for operational efficiency,Secure AI tools,AI expertise for mid-market businesses,Endeavor"><link type=text/plain rel=author href=https://rotational.io/humans.txt><meta property="og:title" content="Streaming NLP Analytics Made Easy With HuggingFace LLMs and Ensign"><meta property="og:description" content="Walk through the end-to-end process of building a streaming sentiment analysis application using a HuggingFace LLM and Ensign for real time predictions."><meta property="og:image" content="https://rotational.io/img/blog/egret-and-buffalo.jpg"><meta property="og:url" content="https://rotational.io/blog/streaming-nlp-with-llms-and-ensign/"><meta property="og:type" content="website"><meta name=twitter:title content="Streaming NLP Analytics Made Easy With HuggingFace LLMs and Ensign"><meta name=twitter:card content="summary"><meta name=twitter:description content="Walk through the end-to-end process of building a streaming sentiment analysis application using a HuggingFace LLM and Ensign for real time predictions."><meta name=twitter:image content="https://rotational.io/img/blog/egret-and-buffalo.jpg"><link rel="shortcut icon" href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=icon href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=alternate type=application/rss+xml href=https://rotational.io//index.xml title="Recent Rotations of the Rotational Labs Blog"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" rel=stylesheet><link rel=stylesheet href=https://rotational.io/output.css media=screen><script src="https://www.googletagmanager.com/gtag/js?id=G-2FKX6CWJHW" defer></script><script type=module>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'G-2FKX6CWJHW');
</script><script src=https://kit.fontawesome.com/fea17a4e21.js crossorigin=anonymous></script></head><body><div class="relative bg-[#1D65A6]"><nav class="w-full max-w-7xl mx-auto px-4 sm:px-6 py-5" aria-label=Global><div class="relative max-w-7xl w-full flex flex-wrap lg:flex-nowrap items-center justify-between mx-auto"><a href=/><img src=/img/rototational-white-text-only_hu15583614935666962927.webp alt=Rotational class="h-6 w-auto">
</a><button data-collapse-toggle=navbar-dropdown type=button class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls=navbar-dropdown aria-expanded=false>
<span class=sr-only>Open main menu</span>
<i class="fa fa-bars text-xl text-white"></i></button><div class="hidden absolute z-[9999] lg:static top-16 w-[92%] md:w-[96%] lg:w-auto lg:block bg-white lg:bg-transparent text-[#192E5B] lg:text-[#F2F2F2] py-2 rounded-md" id=navbar-dropdown><ul class="flex flex-col lg:gap-2 xl:gap-4 md:flex-row font-semibold w-full md:justify-between"><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/about/>About</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><button id=dropdownNavbarLink data-dropdown-toggle=dropdownNavbar class="uppercase flex items-center justify-between gap-x-2 w-full px-3 rounded md:border-0 md:p-0 md:w-auto hover:text-black">
Services
<i class="fa fa-angle-down pt-1"></i></button><div id=dropdownNavbar class="z-10 hidden font-normal bg-[#192E5B] divide-y divide-[#192E5B] rounded-lg shadow w-44"><ul class="py-2 text-sm text-[#F2F2F2]" aria-labelledby=dropdownLargeButton><li><a href=/services/ai-assessments/ class="block px-3 py-2 hover:font-bold">AI Assessments</a></li><li><a href=/services/ai-product-development/ class="block px-3 py-2 hover:font-bold">AI Product Development</a></li><li><a href=/services/ai-ops-and-data-foundations/ class="block px-3 py-2 hover:font-bold">AI Ops & Data Foundations</a></li></ul></div></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/case-studies>Case Studies</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/blog/>Blog</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/learning/>Learning</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/endeavor/>Product</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/contact/>Contact</a></li></ul></div></div></nav></div><main><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class=mt-14><div class=blog-img><img src=/img/blog/egret-and-buffalo_hu10304077854940443591.webp alt="Streaming NLP Analytics Made Easy With HuggingFace LLMs and Ensign" class="mx-auto object-cover"></div><div class=mt-8><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center" data-blog-title="Streaming NLP Analytics Made Easy With HuggingFace LLMs and Ensign"><b class=text-[#1D65A6]>Streaming </b>NLP Analytics Made Easy With HuggingFace LLMs and Ensign</h3><div class="flex flex-wrap justify-center items-center my-4"><a href=/authors/rotational-labs><img src=img/butterfly.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span><a href=/authors/rotational-labs>Rotational Labs</a> | Tuesday, Aug 1, 2023 |&nbsp;
</span><span><a href=/tags/mlops>MLOps</a>,&nbsp;
</span><a href=/tags/eventing>Eventing</a>,&nbsp;
</span><a href=/tags/ai>AI</a>,&nbsp;
</span><a href=/tags/ml>ML</a></span></div><article class="max-w-[800px] mx-auto prose"><p>Thinking about using a large language model (LLM) at your organization? Check out this tutorial to see how to bootstrap an MVP using an open source pre-trained model from <a href=https://huggingface.co/>HuggingFace</a> and a <a href=https://rotational.app/register/>free Ensign account</a>.</p><p><em>&ldquo;How can I build my own LLM? ChatGPT is a black box; I canâ€™t customize it.&rdquo; &ldquo;Donâ€™t LLMs need trillions of samples? We donâ€™t have enough data.&rdquo; &ldquo;LLMs are too complicated - I donâ€™t even know where to begin.&rdquo;</em></p><p>Sound familiar? This is for anyone out there who knows they have a good use case for LLMs but has found themselves stymied. Trust us. You can totally do this. In this post, we will walk through an end-to-end solution for bootstrapping an LLM and deploying it for real time classification.</p><iframe width=560 height=315 src="https://www.youtube.com/embed/7hXMpwS86Ro?start=331" title="YouTube video player" frameborder=0 allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe><p>ðŸŽ¥ You can also watch an <a href="https://www.youtube.com/watch?v=7hXMpwS86Ro&t=331s">on-demand recording of this tutorial</a>, delivered by Rotational Labs Distributed Systems Engineer, <a href=https://rotational.io/authors/prema-roman/>Prema Roman</a>.</p><p>ðŸ˜Ž Just here for Prema&rsquo;s code? <a href=https://github.com/rotationalio/huggingface-example>Here&rsquo;s the repo</a>!</p><h2 id=a-very-relatable-use-case-sentiment-classification>A Very Relatable Use Case (Sentiment Classification)</h2><p>Imagine you work in the Customer Success Department at a large seafood chain restaurant. Customer complaints are being addressed way too slowly, sometimes a week or two after the fact. Serious complaints need to be flagged for Legal because they require mitigation to avoid lawsuits and negative public opinion. But your restaurant serves 85M customers every year and the helpdesk is drowning in complaints that come through phone and email&mldr; not to mention those that are posted on social media.</p><p>Yikes.</p><p>But then you realize you can use an LLM to analyze the social media posts and bucket them into positive and negative reviews and automatically triage them much faster!</p><h2 id=designing-the-solution>Designing the Solution</h2><p>When designing data science applications, it&rsquo;s important to remember that 87% of data science projects <a href=https://venturebeat.com/ai/why-do-87-of-data-science-projects-never-make-it-into-production/>never make it to production</a>. Teams that are new to operationalizing ML have a tendency to make premature decisions that fail to foster organizational buy-in, cause scaling/operations problems later on, or make deployment unnecessarily challenging.</p><p>So when it&rsquo;s time to build a data science application, it&rsquo;s important to take a step back and consider the specific use case and organizational context at hand. If you/your team are new to applications development, it&rsquo;s worth it to cultivate allies from the Engineering team who can help advise you. Application design is not something that most of us learn in data science or machine learning education (from our experience, it&rsquo;s not something that&rsquo;s typically taught even in computer science programs). Design is hard, because it depends on having some intuition about things that can go wrong in an application, which gets easier the more experience you have.</p><p>How else can you prevent your project from being one of the 87% of unsuccessful ones?</p><ul><li>Keep the end goal in mind and prioritize the success of that objective over ancillary ambitions</li><li>Donâ€™t strive for perfection, iterate (take a cue from Engineering projects)</li><li>Feedback loops are important! This helps you with iterating your solution. Feedback can mean both automated feedback (e.g. continuous monitoring and evaluation), and human feedback (i.e. from the people who are the customers of the solution)</li><li>Communication is key. Understand the objectives and pain points of the other teams you work with</li><li>Work with engineering and platform teams to build good development practices to ensure that you have maintainable code and processes</li></ul><h2 id=data-annotation--architecture>Data Annotation & Architecture</h2><p>So we collaborate with our friend from Engineering to sketch out an Application Overview (left side of the below image). Now let&rsquo;s take a crack at architecting the NLP internals and modeling component of our solution.</p><p><img alt="A proposed solution with LLMs and Ensign" src=/img/blog/designing-the-solution.png></p><p>This is the basic blueprint for our streaming analytics/inference application.</p><p>As with all classification problems, the first step is to get labeled data. For this hypothetical use cases, let&rsquo;s say you team up with some colleagues from the Customer Service team, gather all of the relevant the social media posts you can find, and work together to label them as positive or negative. That will give you a label which the application can use to train the supervised model. Each model you train you will then store together with model metric metadata in your chosen storage tool.</p><p>Next, you can start to set up a routine ingestion process to ingest social media posts <em>as soon as</em> the customer posts something on social media. Data ingestion is managed using the pub/sub design pattern, so that we can read in new data (customer reviews) and generate predictions immediately. Predictions will enable automated labeling of reviews as positive and negative, and all negative reviews will get triaged for dispute resolution and positive reviews will get routed to the customer engagement team.</p><h2 id=leveraging-open-source>Leveraging Open Source</h2><p>There are many open source tools available to get you started. In this tutorial, we&rsquo;ll focus on two:</p><ul><li><a href=https://huggingface.co/>HuggingFace</a> has a large selection of pre-trained models, including ones from Google, Microsoft, and Meta, that you can choose from for your specific task. You can also further customize these pre-trained models by feeding your own data, a process called <strong>transfer learning</strong>, and use your domain knowledge and data science intuition to tune the knobs of these models to achieve the outcomes you are looking for.</li><li><a href=https://ensign.rotational.dev/>Ensign</a> is a distributed eventing system. It stores chronological changes to all objects, which you can query with SQL to get change vectors in addition to static snapshots. It can be used to help with ingestion, training, deployment, and continuous evaluation. For this example, we&rsquo;ll use Ensign&rsquo;s <a href=https://github.com/rotationalio/pyensign>Python SDK</a> to deploy the pretrained HuggingFace model on a stream to transform it into a <strong>real time prediction generator</strong>.</li></ul><h2 id=bootstrapping-the-llm>Bootstrapping the LLM</h2><p>The sentiment analysis application will use the DistilBERT model from HuggingFace. The components of the HuggingFace training process include the <code>TrainingArguments</code> (parameters to pass to the <code>Trainer</code> class), and the <code>Trainer</code>.</p><p>The first step for bootstrapping our LLM is to prepare our <code>TrainingArguments</code>. This will allow us to take the pre-trained model and augment it with whatever labeled training data we have been able to acquire/annotate (it takes less than you might think).</p><p>The <code>TrainingArguments</code> is a essentially a dictionary with a huge list of learning parameters. We&rsquo;ll pass these params as well as the directory where we want to save the models to after each run.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>train_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;trained_models&#34;</span>
</span></span><span style=display:flex><span>params<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;learning_rate&#34;</span>: <span style=color:#ae81ff>2e-5</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;per_device_train_batch_size&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;per_device_eval_batch_size&#34;</span>: <span style=color:#ae81ff>4</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;num_train_epochs&#34;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;optim&#34;</span>: <span style=color:#e6db74>&#34;adamw_torch&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;weight_decay&#34;</span>: <span style=color:#ae81ff>0.01</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;save_strategy&#34;</span>: <span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;evaluation_strategy&#34;</span>: <span style=color:#e6db74>&#34;epoch&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;metric_for_best_model&#34;</span>: <span style=color:#e6db74>&#34;f1_true&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;load_best_model_at_end&#34;</span>: <span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;push_to_hub&#34;</span>: <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>training_args <span style=color:#f92672>=</span> TrainingArguments(<span style=color:#f92672>**</span>params,output_dir<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>train_dir)
</span></span></code></pre></div><p>Next, we initialize the pre-trained model and tokenizer using the DistilBert model assets available from HuggingFace. <em>Note: If you were to want to use a different tokenizer or a different model, all you have to do is change this name.</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>self<span style=color:#f92672>.</span>tokenizer<span style=color:#f92672>=</span>DistilBertTokenizer<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>self<span style=color:#f92672>.</span>model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;Distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><p>Now we want to tokenize this data set before we feed it to the model. In this case, imagine we have a Pandas dataframe in this in this case, so we&rsquo;re going to convert it to a <code>Dataset</code> object, which is a HuggingFace class. We will apply this pre-processing function to both the train and the test data.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>preprocess_function</span>(self, instances):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>tokenizer(
</span></span><span style=display:flex><span>        instances[<span style=color:#e6db74>&#34;text&#34;</span>], truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;max_length&#34;</span>
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span><span style=color:#75715e># convert from pandas dataframe to a Dataset object</span>
</span></span><span style=display:flex><span>train_dataset <span style=color:#f92672>=</span> Dataset<span style=color:#f92672>.</span>from_pandas(train_df)
</span></span><span style=display:flex><span>test_dataset <span style=color:#f92672>=</span> Dataset<span style=color:#f92672>.</span>from_pandas(test_df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Use the preprocess_function to tokenize the data</span>
</span></span><span style=display:flex><span>tokenized_train <span style=color:#f92672>=</span> train_dataset<span style=color:#f92672>.</span>map(
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>preprocess_function, batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>tokenized_test <span style=color:#f92672>=</span> test_dataset<span style=color:#f92672>.</span>map(self<span style=color:#f92672>.</span>preprocess_function, batched<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p><em>Note: By setting <code>batched=True</code>, we can ensure that this pre-processing happens in batches.</em></p><p>Next, we set up the <code>Trainer</code> class and run the training process:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer <span style=color:#f92672>=</span> Trainer(
</span></span><span style=display:flex><span>    model<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>model,
</span></span><span style=display:flex><span>    args<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>training_args,
</span></span><span style=display:flex><span>    train_dataset<span style=color:#f92672>=</span>tokenized_train,
</span></span><span style=display:flex><span>    eval_dataset<span style=color:#f92672>=</span>tokenized_test,
</span></span><span style=display:flex><span>    tokenizer<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>tokenizer,
</span></span><span style=display:flex><span>    compute_metrics<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>generate_metrics,
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>metrics <span style=color:#f92672>=</span> trainer<span style=color:#f92672>.</span>evaluate()
</span></span><span style=display:flex><span>print(metrics)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># save the best model based on evaluation metrics</span>
</span></span><span style=display:flex><span>trainer<span style=color:#f92672>.</span>save_model(self<span style=color:#f92672>.</span>model_dir)
</span></span></code></pre></div><p>Congratulations! You have officially bootstrapped your own LLM!</p><h2 id=enabling-real-time-predictions>Enabling Real Time Predictions</h2><p>Now it&rsquo;s time to enable real time predictions. We need to set up a stream so that data can flow into and out of the model we bootstrapped in the previous section.</p><p>For this we will use the concept of <code>Publishers</code> and <code>Subscribers</code> from <a href=https://ensign.rotational.dev/><code>Ensign</code></a>, an open source eventing platform for data scientists:</p><p>Our <code>ScoreDataPublisher</code> will send data to our model:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ScoreDataPublisher</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, topic<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;yelp_data&#34;</span>, interval<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>topic <span style=color:#f92672>=</span> topic
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ensign <span style=color:#f92672>=</span> Ensign()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>interval <span style=color:#f92672>=</span> interval
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run</span>(self):
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>get_event_loop()<span style=color:#f92672>.</span>run_until_complete(self<span style=color:#f92672>.</span>publish())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>publish</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>ensure_topic_exists(self<span style=color:#f92672>.</span>topic)
</span></span><span style=display:flex><span>        score_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#34;data&#34;</span>, <span style=color:#e6db74>&#34;yelp_score.csv&#34;</span>))
</span></span><span style=display:flex><span>        score_dict <span style=color:#f92672>=</span> score_df<span style=color:#f92672>.</span>to_dict(<span style=color:#e6db74>&#34;records&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> record <span style=color:#f92672>in</span> score_dict:
</span></span><span style=display:flex><span>            event <span style=color:#f92672>=</span> Event(
</span></span><span style=display:flex><span>                json<span style=color:#f92672>.</span>dumps(record)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>),
</span></span><span style=display:flex><span>                mimetype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>topic,
</span></span><span style=display:flex><span>                event,
</span></span><span style=display:flex><span>                on_ack<span style=color:#f92672>=</span>print_ack,
</span></span><span style=display:flex><span>                on_nack<span style=color:#f92672>=</span>print_nack
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>sleep(self<span style=color:#f92672>.</span>interval)
</span></span></code></pre></div><p>Next, we&rsquo;ll build our predictor, the <code>HuggingFaceScorer</code>, which will allow our application to subscribe to the <code>yelp_data</code> topic and asynchronously generate predictions:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>HuggingFaceScorer</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, topic<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;yelp_data&#34;</span>, model_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;final_model&#34;</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>topic <span style=color:#f92672>=</span> topic
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_dir <span style=color:#f92672>=</span> model_dir
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ensign <span style=color:#f92672>=</span> Ensign()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>load_model()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run</span>(self):
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>get_event_loop()<span style=color:#f92672>.</span>run_until_complete(self<span style=color:#f92672>.</span>subscribe())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_model</span>(self):
</span></span><span style=display:flex><span>        tokenizer <span style=color:#f92672>=</span> DistilBertTokenizer<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;distilbert-base-uncased-finetuned-sst-2-english&#34;</span>
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        model <span style=color:#f92672>=</span> AutoModelForSequenceClassification<span style=color:#f92672>.</span>from_pretrained(
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>model_dir
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>classifier <span style=color:#f92672>=</span> pipeline(
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#34;sentiment-analysis&#34;</span>,
</span></span><span style=display:flex><span>            model<span style=color:#f92672>=</span>model,
</span></span><span style=display:flex><span>            tokenizer<span style=color:#f92672>=</span>tokenizer
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_predictions</span>(self, data):
</span></span><span style=display:flex><span>        text_list <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        text <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;text&#34;</span>]
</span></span><span style=display:flex><span>        text_list<span style=color:#f92672>.</span>append(text)
</span></span><span style=display:flex><span>        pred_info <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>classifier(text_list)
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;NEGATIVE&#34;</span> <span style=color:#f92672>in</span> pred_info[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;label&#34;</span>] <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        pred_score <span style=color:#f92672>=</span> pred_info[<span style=color:#ae81ff>0</span>][<span style=color:#e6db74>&#34;score&#34;</span>]
</span></span><span style=display:flex><span>        label <span style=color:#f92672>=</span> data[<span style=color:#e6db74>&#34;labels&#34;</span>]
</span></span><span style=display:flex><span>        print(text)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;prediction: </span><span style=color:#e6db74>{</span>pred<span style=color:#e6db74>}</span><span style=color:#e6db74>, prediction_score: </span><span style=color:#e6db74>{</span>pred_score<span style=color:#e6db74>}</span><span style=color:#e6db74>, label: </span><span style=color:#e6db74>{</span>label<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>subscribe</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>ensure_topic_exists(self<span style=color:#f92672>.</span>topic)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>subscribe(self<span style=color:#f92672>.</span>topic):
</span></span><span style=display:flex><span>            data <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>decode(event)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>generate_predictions(data)
</span></span></code></pre></div><p>To test drive your real time sentiment classification application, run your <code>HuggingFaceScorer</code> in one terminal, and your <code>ScoreDataPublisher</code> in a separate terminal.</p><p>That&rsquo;s it! You can watch the video <a href="https://www.youtube.com/watch?v=7hXMpwS86Ro&t=331s">here</a> and find all the code <a href=https://github.com/rotationalio/huggingface-example>here</a>.</p><h2 id=conclusion>Conclusion</h2><p>As you&rsquo;ve seen in this post, you can start incrementally delivering insights to your organization without needing to resort to purchasing a pricey proprietary solution, or waiting to accumulate a trillion rows of data. Using the bootstrapping pattern and open source, Python-friendly resources like HuggingFace and Ensign, you can empower your roadmap â€” youâ€™re set up to implement your own custom LLM by simply swapping out the data and model (no tech debt required).</p><p>Ready to take the next step? Check out <a href="https://youtu.be/w69glRpOBD4?si=7V6vYUZa3uPBbXXy">MLOps 201: Data Flows for Real Time Model Inferencing</a>.</p><p>Ready to experiment with data streams and change data capture? Check out <a href=https://rotational.io/data-playground/>The Data Playground</a> and set up your own <a href=https://rotational.app/register/>free Ensign account</a>.</p><div class="border-t my-12"></div><p>Photo by <a href=https://www.flickr.com/photos/francesco_veronesi/>Francesco Veronesi</a> on <a href=https://flic.kr/p/2ne6M5j>Flickr Commons</a></p></article></div></div><div class="bg-[#E8EFF6] max-w-[800px] mx-auto mt-9 p-8 rounded-lg"><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center mb-3"><span class="text-[#1D65A6] font-bold">About </span>This Post</h3><p class="text-base mx-auto px-2 text-center lg:text-base">Walk through the end-to-end process of building a streaming sentiment analysis application using a HuggingFace LLM and Ensign for real time predictions.</p><div class="flex flex-col md:flex-row text-center mx-auto border-t pt-6 mt-6 align-center justify-between gap-10"><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Written by:</h2><div class="flex items-center"><a href=/authors/rotational-labs><img src=img/butterfly.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span class="flex flex-wrap"><a href=/authors/rotational-labs class="lg:w-[20ch] mx-2">Rotational Labs</a></span></div></div><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Share this post:</h2><ul class="flex items-center justify-center gap-6 mt-4"><li><a onclick=shareByEmail() class=cursor-pointer><img src=img/email.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnTwitterWithTitle() class=cursor-pointer><img src=img/twitter.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnLinkedIn() class=cursor-pointer><img src=img/linkedin.png alt class="rounded-lg bg-white p-3"></a></li></ul></div></div></div><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class="flex justify-between mt-12 sm:mt-24 items-center"><div class="flex items-center"><h2 class="font-bold text-2xl sm:text-4xl flex"><span class=text-[#1D65A6]>Recommended</span>
&nbsp;Rotations</h2></div><div><a href=/blog class="flex text-base sm:text-lg items-center font-bold text-[#1D65A6]"><span>View all</span>
<img src=img/arr-right.png alt class="h-4 ml-2"></a></div></div><div><ul class="grid sm:grid-cols-2 lg:grid-cols-3 gap-6 sm:mt-16"><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/what-is-incremental-machine-learning/><img loading=lazy src=/img/blog/festival-lights_hu9134923142592729850.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/nlp>NLP</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/eventing>Eventing</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/what-is-incremental-machine-learning/ class=block>What is Incremental Machine Learning?</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">Most of us got into data science because it&rsquo;s exciting (if chaotic) and there&rsquo;s a constant stream of new ideas, which is thrilling (if intimidating). But if learning is all about keeping up, why can&rsquo;t our models do it?</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=img/team/aatmaj-janardanan.png alt class=rounded-full><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/aatmaj-janardanan>Aatmaj Janardanan</a></li></ul></div><div class=font-extralight>Aug 21, 2023</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/realtime-machine-learning/><img loading=lazy src=/img/blog/2023-08-07-realtime-machine-learning/bird-fly-unsplash_hu7209337365396706980.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/nlp>NLP</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/eventing>Eventing</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/realtime-machine-learning/ class=block>Using River and Vowpal Wabbit to Build Real-Time Machine Learning Models</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">Real-time ML models continually learn on new data as soon as it arrives, so they&rsquo;re less susceptible to concept drift and data drift. Read on to learn how to use River and Vowpal Wabbit to build real-time models in Python.</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=img/team/prema-roman.png alt class=rounded-full><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>Aug 11, 2023</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/mlops-through-eventing/><img loading=lazy src=/img/blog/airshow_hu13860938531206189505.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/mlops>MLOps</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/eventing>Eventing</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/mlops-through-eventing/ class=block>MLOps 101: A Fresh Approach to Managing Models with Event Streams</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">If you can&rsquo;t deploy your models, you might feel frustrated, but you aren&rsquo;t alone â€” only 1 in 10 data science projects ever gets deployed. The fix? We need to shift our mental model towards &ldquo;thinking in events.&rdquo;</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=img/butterfly.png alt class=rounded-full><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/rotational-labs>Rotational Labs</a></li></ul></div><div class=font-extralight>Jul 11, 2023</div></div></div></li></ul></div></div></div><div class="bg-[#1D65A6] max-w-[800px] mx-auto mt-20 py-14 px-12 md:px-16 text-white md:rounded-lg"><form action=blog method=post id=newsletterForm><h6 class="font-bold text-center">Enter Your Email To Subscribe</h6><label for=email class=hidden>Email</label>
<input type=text name=email id=email required placeholder class="w-full px-4 py-2.5 rounded-lg mt-6 text-black" style=color:#000><div class="flex mt-6 items-start gap-x-2"><input type=checkbox id=checkbox required class="mt-1 w-4 h-4 block border-0">
<label for=checkbox><span>I want to receive the monthly newsletter and other updates from Rotational. You agree to our Privacy Policy. You may unsubscribe at any time.*</span></label></div><div class="bg-teal-100 border-t-4 border-teal-500 mt-10 rounded-b text-teal-900 px-4 py-3 shadow-md hidden" id=newsletter-alert role=alert><div class=flex><div><p class=text-sm>Thank you for your interest!</p></div></div></div><div class="flex justify-center"><button type=submit class="bg-[#192E5B] px-14 py-4 mt-10 rounded-lg text-sm text-white uppercase md:text-base">
Submit</button></div></form></div></main><footer class="relative mt-40 md:mt-56 bg-[#192E5B]"><div class="relative w-full pt-36 md:pt-16 lg:pt-24 2xl:pt-20 font-extralight text-white"><div class="-mt-52 w-full mx-auto max-w-screen-xl px-4"><section class="bg-[#72A2C0] w-full p-6 md:py-20 md:px-16"><h2 class="my-4 text-2xl sm:text-3xl md:text-5xl text-white font-extrabold">LET'S ENVISION & BUILD THE FUTURE TOGETHER.</h2><div class=py-6><a href=/contact class="p-3 md:p-4 md:px-6 bg-[#2F4858] font-bold md:text-lg text-white text-center hover:bg-[#2F4858]/80">CONTACT US</a></div></section></div><div class="max-w-7xl mx-auto px-6"><div class="mt-12 flex flex-col md:flex-row lg:justify-between gap-x-8"><div class="my-4 max-w-xs"><h5 class="mb-3 font-extrabold">OUR PRESENCE</h5><p>We share because we care, about topics, tools, and technologies that we believe impact the AI economy.</p><div class=py-4><ul class="flex justify-between items-center gap-x-8"><li><a href=https://twitter.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-x-twitter"></i><p class=sr-only>Twitter</p></a></li><li><a href=https://www.linkedin.com/company/rotational target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-linkedin"></i><p class=sr-only>LinkedIn</p></a></li><li><a href=https://github.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-github"></i><p class=sr-only>GitHub</p></a></li><li><a href=https://www.youtube.com/@rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-youtube"></i><p class=sr-only>YouTube</p></a></li><li><a href=https://www.twitch.tv/rotationallabs target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-twitch"></i><p class=sr-only>Twitch</p></a></li></ul></div></div><div class=my-4><h5 class="mb-3 font-extrabold">COMPANY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/about>About Us</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/case-studies>Case Studies</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/endeavor>Endeavor</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/blog>Blog</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">COMMUNITY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/learning>Learning</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/opensource>Open Source</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">CONTACT US</h5><ul><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-map-marker-alt text-[#757575]"></i>
St. Paul, MN & Washington, DC</li><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-envelope text-[#757575]"></i>
info@rotational.io</li></ul><div class=py-8><a href=/contact class="p-3 bg-[#ECF6FF] font-bold text-black text-center hover:bg-[#ECF6FF]/80">CONTACT US</a></div></div></div><div class="sm:flex justify-between py-6 border-t mt-4"><p>Copyright Â© Rotational Labs, Inc. 2021â€“2024 Â· All Rights Reserved</p><div><ul class="sm:mt-0 mt-4 flex gap-x-8"><li><a href=/privacy/>Privacy Policy</a></li><li><a href=/terms/>Terms of Use</a></li></ul></div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/flowbite@2.5.2/dist/flowbite.min.js></script><script type=text/javascript id=hs-script-loader async defer src=//js.hs-scripts.com/24168101.js></script><script src="https://www.google.com/recaptcha/enterprise.js?render=6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk"></script><script>grecaptcha.enterprise.ready(function(){grecaptcha.enterprise.execute("6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk",{action:"homepage"}).then(function(){})})</script><script src=https://rotational.io/js/blogSingle.js></script></body></html>