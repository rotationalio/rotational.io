<!doctype html><html lang=en-us><head><meta charset=UTF-8><title>Rotational Labs | Monitoring Real-Time Machine Learning Applications With Prefect</title>
<base href=https://rotational.io/ target=_self><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rotational Labs, Inc."><meta name=description content="Prefect enables data scientists to add orchestration and observability into their data pipelines without having to write custom code."><meta name=keywords content="AI solutions for mid-market companies,AI-driven tools,Business automation solutions,AI-powered interfaces,Intelligent agents,Natural language processing (NLP),Computer vision AI,Streamline operations with AI,machine learning,Boost business performance with AI,AI for business growth,Tailored AI solutions,Trusted AI solutions,AI for operational efficiency,Secure AI tools,AI expertise for mid-market businesses,Endeavor"><link type=text/plain rel=author href=https://rotational.io/humans.txt><meta property="og:title" content="Monitoring Real-Time Machine Learning Applications With Prefect"><meta property="og:description" content="Prefect enables data scientists to add orchestration and observability into their data pipelines without having to write custom code."><meta property="og:image" content="https://rotational.io/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/orchestra.webp"><meta property="og:url" content="https://rotational.io/blog/monitor-real-time-ml-apps-with-prefect/"><meta property="og:type" content="website"><meta name=twitter:title content="Monitoring Real-Time Machine Learning Applications With Prefect"><meta name=twitter:card content="summary"><meta name=twitter:description content="Prefect enables data scientists to add orchestration and observability into their data pipelines without having to write custom code."><meta name=twitter:image content="https://rotational.io/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/orchestra.webp"><link rel="shortcut icon" href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=icon href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=alternate type=application/rss+xml href=https://rotational.io//index.xml title="Recent Rotations of the Rotational Labs Blog"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" rel=stylesheet><link rel=stylesheet href=https://rotational.io/output.css media=screen><script src="https://www.googletagmanager.com/gtag/js?id=G-2FKX6CWJHW" async defer></script><script type=module>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'G-2FKX6CWJHW');
</script><script async defer src="https://www.google.com/recaptcha/enterprise.js?render=6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk"></script><script src=https://kit.fontawesome.com/fea17a4e21.js crossorigin=anonymous></script></head><body><div class="relative bg-[#1D65A6]"><nav class="w-full max-w-7xl mx-auto px-4 sm:px-6 py-5" aria-label=Global><div class="relative max-w-7xl w-full flex flex-wrap lg:flex-nowrap items-center justify-between mx-auto"><a href=/><img src=/img/rototational-white-text-only_hu15583614935666962927.webp alt=Rotational class="h-6 w-auto">
</a><button data-collapse-toggle=navbar-dropdown type=button class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls=navbar-dropdown aria-expanded=false>
<span class=sr-only>Open main menu</span>
<i class="fa fa-bars text-xl text-white"></i></button><div class="hidden absolute z-[9999] lg:static top-16 w-[92%] md:w-[96%] lg:w-auto lg:block bg-white lg:bg-transparent text-[#192E5B] lg:text-[#F2F2F2] py-2 rounded-md" id=navbar-dropdown><ul class="flex flex-col lg:gap-2 xl:gap-4 md:flex-row font-semibold w-full md:justify-between"><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/about/>About</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><button id=dropdownNavbarLink data-dropdown-toggle=dropdownNavbar class="uppercase flex items-center justify-between gap-x-2 w-full px-3 rounded md:border-0 md:p-0 md:w-auto hover:text-black">
Services
<i class="fa fa-angle-down pt-1"></i></button><div id=dropdownNavbar class="z-10 hidden font-normal bg-[#192E5B] divide-y divide-[#192E5B] rounded-lg shadow w-44"><ul class="py-2 text-sm text-[#F2F2F2]" aria-labelledby=dropdownLargeButton><li><a href=/services/ai-assessments/ class="block px-3 py-2 hover:font-bold">AI Assessments</a></li><li><a href=/services/ai-product-development/ class="block px-3 py-2 hover:font-bold">AI Product Development</a></li><li><a href=/services/ai-ops-and-data-foundations/ class="block px-3 py-2 hover:font-bold">AI Ops & Data Foundations</a></li></ul></div></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/case-studies>Case Studies</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/blog/>Blog</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/learning/>Learning</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/endeavor/>Product</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/contact/>Contact</a></li></ul></div></div></nav></div><main><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class=mt-14><div class=blog-img><img src=/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/orchestra_hu9125066911412800891.webp alt="Monitoring Real-Time Machine Learning Applications With Prefect" class="mx-auto object-cover"></div><div class=mt-8><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center" data-blog-title="Monitoring Real-Time Machine Learning Applications With Prefect"><b class=text-[#1D65A6]>Monitoring </b>Real-Time Machine Learning Applications With Prefect</h3><div class="flex flex-wrap justify-center items-center my-4"><a href=/authors/prema-roman><img src=img/team/prema-roman.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span><a href=/authors/prema-roman>Prema Roman</a> | Friday, Jan 26, 2024 |&nbsp;
</span><span><a href=/tags/ai>AI</a>,&nbsp;
</span><a href=/tags/ml>ML</a>,&nbsp;
</span><a href=/tags/python>Python</a>,&nbsp;
</span><a href=/tags/data>Data</a></span></div><article class="max-w-[800px] mx-auto prose"><p>The last few years have marked a shift in industry away from research-oriented machine learning. These days, shipping models early and often is critical, and open source tools like Prefect can speed up the process of operationalizing ML.</p><p>2024 finds us an AI arms race, where companies are rushing to get value out of AI and feeling the pressure to deploy solutions quickly so that they don’t get scooped by competitors.</p><p>For those of us from a more traditional data science background where a deep understanding of the algorithms was the main focus of our education, shifting to this new paradigm has caused growing pains. It already takes an inordinate amount of time working with data and making sense of it to build a machine learning model, let alone having to now manage application development and deployments.</p><p>Fortunately, there are many new tools that are now available that take away some of the complexities around operationalizing and managing machine learning models. <a href=https://www.prefect.io>Prefect</a> is a modern workflow orchestration platform designed specifically for data practitioners to help them manage their workflows and to add observability into their applications without having to write custom code. Even better, it&rsquo;s written in Python, which makes it feel more approachable than other devOps/MLOps tools.</p><p><em>Just here for the code? Check out the git repo <a href=https://github.com/rotationalio/prefect-example>here</a>.</em></p><h2 id=getting-started>Getting Started</h2><p>Prefect offers a lot of neat features but what’s even more awesome is that getting started is SUPER easy. All it takes to integrate Prefect into your application workflow is four steps:</p><ol><li>Install Prefect in a virtual environment like you would any other package: <code>pip install prefect</code>.</li><li>For functions within your application that you want to monitor, e.g., <code>score</code> for monitoring model predictions, add the <code>@flow</code> decorator on top of the function signature.</li><li>Run the following command in a terminal window: <code>prefect server start</code> (note that your virtual environment must be activated before running this command).</li><li>Run your application and view the live status of the application in the Prefect dashboard.</li></ol><p>The example I will demonstrate is a real-time sentiment analysis application that uses <a href=https://rotational.io/ensign/>Ensign</a> as the streaming data platform, <a href=https://riverml.xyz>River</a> for the real-time machine learning model, and <a href=https://www.prefect.io>Prefect</a> as the orchestration platform. We&rsquo;ll use this sample dataset of <a href=https://github.com/rotationalio/prefect-example/tree/main/data>Yelp reviews</a> as a stand-in for the data streaming into our to-be-deployed model.</p><p>For those of you who have read some of my previous blog posts, you will recognize that the code that I am using is eerily similar to one that I used in an earlier blog post &#x1f61c;. This proves that it doesn’t take much to integrate Prefect into an existing application.</p><p>If you are not familiar with Ensign, check out <a href=https://rotational.io/ensign-u/>Ensign U</a> to learn more about how you can use Ensign to build real-time applications. For this application you will need to create two topics: <code>river_pipeline</code> and <code>river_metrics</code>. Topics are analogous to tables in a relational database.</p><h2 id=architecture>Architecture</h2><p>This application consists of three components:</p><ul><li>A publisher component that reads data from a csv file that contains review data and publishes (writes) this data to the <code>river_pipeline</code> topic.</li><li>A subscriber component that listens for new messages on the <code>river_pipeline</code> topic and loads and uses a real-time machine learning model that makes predictions and learns incrementally as it receives new training instances. At the end, it calculates the precision and recall scores which it publishes to a different topic called <code>river_metrics</code>.</li><li>The third component listens for new messages on the <code>river_metrics</code> topic and checks to see if the precision or recall scores are below a pre-specified threshold and logs the values if they fall below the threshold.</li></ul><p>As you can imagine, especially with real-time applications that are going to be running on an ongoing basis, it is crucial that there is monitoring set up so that you can view the health and status of your applications and can address issues as soon as they arise.</p><p>Let’s now take a look at some code and see how to integrate Prefect into the application.</p><h3 id=going-with-the-flow>Going with the Flow</h3><p>The first key component of an operationalized model is a steady stream of input data, ready for training or prediction. In a live application, this could be a batch-wise process, a request-response model, or a real-time feed of streaming updates.</p><p>For this example, let&rsquo;s imagine we have a real-time data feed that we would like to connect our model to. Let&rsquo;s write a publisher class with a method to emulate that real-time feed so we can get put together an end-to-end architecture.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DataPublisher</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Emulate real-time ingestion&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>publish</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Read data from the yelp.csv file and publish to river_pipeline topic.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        This can be replaced by a real time streaming source
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Check out https://github.com/rotationalio/data-playground for examples
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        train_df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(<span style=color:#e6db74>&#34;data&#34;</span>, <span style=color:#e6db74>&#34;yelp.csv&#34;</span>))
</span></span><span style=display:flex><span>        train_dict <span style=color:#f92672>=</span> train_df<span style=color:#f92672>.</span>to_dict(<span style=color:#e6db74>&#34;records&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> record <span style=color:#f92672>in</span> train_dict:
</span></span><span style=display:flex><span>            event <span style=color:#f92672>=</span> Event(json<span style=color:#f92672>.</span>dumps(record)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>), mimetype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(self<span style=color:#f92672>.</span>topic, event, on_ack<span style=color:#f92672>=</span>handle_ack, on_nack<span style=color:#f92672>=</span>handle_nack)
</span></span></code></pre></div><p>This function reads a csv file and converts it to a Pandas dataframe that is then converted to a list of dictionary objects. It loops through this list of dictionary objects one record at a time and converts them into Ensign <code>events</code> that are published (stored) in the <code>river_pipeline</code> topic. In our hypothetical application, this will eventually get swapped out with a real-time source accessed through an API.</p><p>In order to integrate Prefect into this application, we will write another function that instantiates the <code>DataPublisher</code> class and calls <code>publish</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@flow</span>(log_prints<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_data_publisher</span>():
</span></span><span style=display:flex><span>    publisher <span style=color:#f92672>=</span> DataPublisher()
</span></span><span style=display:flex><span>    asyncio<span style=color:#f92672>.</span>run(publisher<span style=color:#f92672>.</span>publish())
</span></span></code></pre></div><p>The <code>flow</code> decorator converts the function into a Prefect <strong>Flow</strong>, which is a special function that can be monitored and can optionally have built-in retries if specified. The <code>log_prints</code> parameter converts the print statements to INFO level log messages. If you want to set up other types of logging messages such as WARNING and ERROR messages, you can use <code>get_run_logger</code> from Prefect. We will see this example in the Metrics Subscriber component.</p><p>The other item to note above is the use of <code>asyncio.run</code> before the function call. Since the publish method is asynchronous we use <code>asyncio</code> to call the function. Asynchronous functions are common in real-time applications where functions need to have the ability to run independently of each other.</p><p>And that’s all it takes to monitor the Data Publisher component!</p><h3 id=serving-data-to-a-real-time-model>Serving Data to a Real-Time Model</h3><p>Now we need to serve that incoming data to our model. Let&rsquo;s create a second class that will subscribe to the data feed and invoke the model whenever a new event (aka data point) arrives.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>DataSubscriber</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Serve data to the model&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>subscribe</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Receive messages from river_pipeline topic
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>for</span> event <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>subscribe(self<span style=color:#f92672>.</span>sub_topic):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>run_model_pipeline(event)
</span></span></code></pre></div><p>The <code>run_model_pipeline</code> function will use the new data to update a real-time version of a Multinomial Naive Bayes classifier that is wrapped in a Pipeline object with a bag-of-words vectorizer.</p><p><em>Note: This uses the concept of online machine learning, which is a good alternative to wholesale batchwise retraining, because it allows the model to learn incrementally. For those not familiar with real-time ML models, this might look backwards!</em>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_model_pipeline</span>(self, event):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Make a prediction and update metrics based on the predicted value
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    and the actual value.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Incrementally learn/update model based on the actual value
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    record <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(event<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>predict_one(record[<span style=color:#e6db74>&#34;text&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> y_pred <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>confusion_matrix<span style=color:#f92672>.</span>update(y_true<span style=color:#f92672>=</span>record[<span style=color:#e6db74>&#34;sentiment&#34;</span>], y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>classification_report<span style=color:#f92672>.</span>update(y_true<span style=color:#f92672>=</span>record[<span style=color:#e6db74>&#34;sentiment&#34;</span>], y_pred<span style=color:#f92672>=</span>y_pred)
</span></span><span style=display:flex><span>    <span style=color:#75715e># the precision and recall won&#39;t be great at first, but as the model learns on</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># new data, the scores improve</span>
</span></span><span style=display:flex><span>    print(self<span style=color:#f92672>.</span>precision_recall)
</span></span><span style=display:flex><span>    pr_list <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>precision_recall<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>    pr_dict <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;precision&#34;</span>: pr_list[<span style=color:#ae81ff>0</span>], <span style=color:#e6db74>&#34;recall&#34;</span>: pr_list[<span style=color:#ae81ff>1</span>]}
</span></span><span style=display:flex><span>    event <span style=color:#f92672>=</span> Event(json<span style=color:#f92672>.</span>dumps(pr_dict)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>), mimetype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(self<span style=color:#f92672>.</span>pub_topic, event, on_ack<span style=color:#f92672>=</span>handle_ack, on_nack<span style=color:#f92672>=</span>handle_nack)
</span></span><span style=display:flex><span>    <span style=color:#75715e># learn from the train example and update the model</span>
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>learn_one(record[<span style=color:#e6db74>&#34;text&#34;</span>], record[<span style=color:#e6db74>&#34;sentiment&#34;</span>])
</span></span></code></pre></div><p>With traditional models, we typically <code>fit</code> a model to a batch of data and then run predictions on new data. Real-time machine learning models make predictions first on new data and then learn on that data when the label becomes available. We have the labels, so we will simply make the prediction first by calling <code>predict_one</code> and then call <code>learn_one</code> on the same data. We will also update the confusion matrix and the classification report using the prediction and the label. Finally, the latest precision and recall scores are published to the <code>river_metrics</code> topic.</p><p>We will integrate Prefect as shown below. Note that it looks almost the same as the function that was used for the Data Publisher component.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@flow</span>(log_prints<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_data_subscriber</span>():
</span></span><span style=display:flex><span>    subscriber <span style=color:#f92672>=</span> DataSubscriber()
</span></span><span style=display:flex><span>    asyncio<span style=color:#f92672>.</span>run(subscriber<span style=color:#f92672>.</span>subscribe())
</span></span></code></pre></div><h3 id=enabling-model-observability>Enabling Model Observability</h3><p>For most machine learning models, we want to be able to track model performance so that we can ensure that it is functioning correcting. Because this model gets updated incrementally as new data flows through, we can instrument a model metrics checker to give us insight into the real-time training process.</p><p>The class used in this component is called <code>MetricsSubscriber</code> and the top level function is also called <code>subscribe</code>. It calls another function called <code>check_metrics</code> which is defined below.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>MetricsSubscriber</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Enable real-time model metrics observability&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># ...</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>check_metrics</span>(self, event):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Check precision and recall metrics and print if below threshold
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        metric_info <span style=color:#f92672>=</span> json<span style=color:#f92672>.</span>loads(event<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>        precision <span style=color:#f92672>=</span> metric_info[<span style=color:#e6db74>&#34;precision&#34;</span>]
</span></span><span style=display:flex><span>        recall <span style=color:#f92672>=</span> metric_info[<span style=color:#e6db74>&#34;recall&#34;</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> precision <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>threshold:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>logger<span style=color:#f92672>.</span>warn(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Precision is below threshold: </span><span style=color:#e6db74>{</span>precision<span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> recall <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>threshold:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>logger<span style=color:#f92672>.</span>warn(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Recall is below threshold: </span><span style=color:#e6db74>{</span>recall<span style=color:#e6db74>}</span><span style=color:#e6db74>”)</span>
</span></span></code></pre></div><p>For each event that arrives on the <code>river_metrics</code> topic, it extracts the precision and recall scores and checks to see if the scores are below a pre-defined threshold. You will notice here that the print statements are replaced by a logger which is instantiated in the class using Prefect’s <code>get_run_logger</code> method. In this case, we want to log a WARNING message so that the person who is monitoring the application can check to see if this is something that needs to be addressed. The following code snippet shows how Prefect is integrated here. You can see how the logger is initialized and passed to the <code>MetricsSubscriber</code> class as a parameter.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#a6e22e>@flow</span>(log_prints<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>run_metrics_subscriber</span>():
</span></span><span style=display:flex><span>    logger <span style=color:#f92672>=</span> get_run_logger()
</span></span><span style=display:flex><span>    subscriber <span style=color:#f92672>=</span> MetricsSubscriber(logger<span style=color:#f92672>=</span>logger)
</span></span><span style=display:flex><span>    asyncio<span style=color:#f92672>.</span>run(subscriber<span style=color:#f92672>.</span>subscribe())
</span></span></code></pre></div><h2 id=running-the-application>Running the Application</h2><p>Now let&rsquo;s put all the pieces together and run our application! Go <a href=https://github.com/rotationalio/prefect-example>here</a> to find the full source code and make sure you have the dependencies needed to run the application.</p><h3 id=step-1-create-and-source-environment-variables-on-your-machine>Step 1: Create and source environment variables on your machine</h3><p>In order to use Ensign, you will need to have downloaded API keys and set them as environment variables as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>export ENSIGN_CLIENT_ID<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;your client id here&#34;</span>
</span></span><span style=display:flex><span>export ENSIGN_CLIENT_SECRET<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;your client secret here&#34;</span>
</span></span></code></pre></div><h3 id=step-2-create-and-activate-a-python-virtual-environment>Step 2: Create and activate a Python virtual environment</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ virtualenv venv
</span></span><span style=display:flex><span>$ source venv/bin/activate
</span></span></code></pre></div><h3 id=step-3-install-the-required-packages>Step 3: Install the required packages</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ pip install -r requirements.txt
</span></span></code></pre></div><h3 id=step-4-set-up-and-start-the-prefect-server>Step 4: Set up and start the Prefect server</h3><p>You can either host your own Prefect server on your own infrastructure or use <a href=https://app.prefect.cloud/>Prefect Cloud</a>. Follow the instructions after signing up to create a Prefect server on the cloud.</p><p>If you choose to set up a local server, you simply need to open up a terminal window and run the following command to get started. Make sure that Prefect is installed in your virtual environment first and that you have activated it. The command to run the Prefect server on your machine is as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ prefect server start
</span></span></code></pre></div><h3 id=step-5-run-the-metrics-subscriber>Step 5: Run the Metrics Subscriber</h3><p>Open up a new terminal window and activate the virtual environment. Run the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ python river_sentiment_analysis.py metrics
</span></span></code></pre></div><h3 id=step-6-run-the-data-subscriber>Step 6: Run the Data Subscriber</h3><p>Open up a new terminal window and activate the virtual environment. Run the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ python river_sentiment_analysis.py subscribe
</span></span></code></pre></div><h3 id=step-7-run-the-data-publisher>Step 7: Run the Data Publisher</h3><p>Open up a new terminal window and activate the virtual environment. Run the following command:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ python river_sentiment_analysis.py publish
</span></span></code></pre></div><p>You can then take a look at the Prefect dashboard to view your jobs. It will look something like the screenshot below. Note that there are three bars below to the <code>Flow Runs</code> section. The two blue bars are the Data Subscriber and the Metrics Subscriber components. These are blue because they are still running. The green bar is the Data Publisher which is green because it finished running and it ran successfully. Since the publisher was reading from a csv file instead of a real-time API, it completed running after there were no more records in the csv file. You can see that the dashboard gives you a quick overview of the health and status of your application.</p><p><img alt="Prefect Dashboard" src=/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/prefect_dashboard.webp></p><p>The following screenshot shows the Flow runs. This gives you the ability to drill down further into a single component.</p><p><img alt="Prefect Flow Runs" src=/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/prefect_flow_runs.webp></p><p>You can also check the logs for warnings and errors. Note you can see the instances where the precision and recall were lower than the threshold. One thing to note with real-time models is they tend to perform poorly at first and then get better over time as is evidenced by the screenshot below.</p><p><img alt="Prefect Logs" src=/img/blog/2024-01-26-monitor-real-time-ml-apps-with-prefect/prefect_logs.webp></p><p>Prefect offers a lot more features not demonstrated in this example such as scheduling, notifications, and artifacts that can be used for data quality checks and documentation. These are particularly useful for analyzing results over time.</p><h2 id=conclusion>Conclusion</h2><p>As you can see, integrating Prefect into an application is very straightforward. But more importantly, it empowers scrappy data teams to be self-reliant by adding observability into their own applications without having to rely on an external team to build one for them. And given the current “data science 2.0” climate where model deployment is becoming even more critical, leveraging a user-friendly tool like Prefect is the ideal solution to ensure that we remain competitive as data scientists.</p><div class="border-t my-12"></div><p>Photo by <a href="https://unsplash.com/@gwundrig?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Manuel Nägeli on Unsplash</a></p></article></div></div><div class="bg-[#E8EFF6] max-w-[800px] mx-auto mt-9 p-8 rounded-lg"><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center mb-3"><span class="text-[#1D65A6] font-bold">About </span>This Post</h3><p class="text-base mx-auto px-2 text-center lg:text-base">Prefect enables data scientists to add orchestration and observability into their data pipelines without having to write custom code.</p><div class="flex flex-col md:flex-row text-center mx-auto border-t pt-6 mt-6 align-center justify-between gap-10"><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Written by:</h2><div class="flex items-center"><a href=/authors/prema-roman><img src=img/team/prema-roman.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span class="flex flex-wrap"><a href=/authors/prema-roman class="lg:w-[20ch] mx-2">Prema Roman</a></span></div></div><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Share this post:</h2><ul class="flex items-center justify-center gap-6 mt-4"><li><a onclick=shareByEmail() class=cursor-pointer><img src=img/email.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnTwitterWithTitle() class=cursor-pointer><img src=img/twitter.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnLinkedIn() class=cursor-pointer><img src=img/linkedin.png alt class="rounded-lg bg-white p-3"></a></li></ul></div></div></div><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class="flex justify-between mt-12 sm:mt-24 items-center"><div class="flex items-center"><h2 class="font-bold text-2xl sm:text-4xl flex"><span class=text-[#1D65A6]>Recommended</span>
&nbsp;Rotations</h2></div><div><a href=/blog class="flex text-base sm:text-lg items-center font-bold text-[#1D65A6]"><span>View all</span>
<img src=img/arr-right.png alt class="h-4 ml-2"></a></div></div><div><ul class="grid sm:grid-cols-2 lg:grid-cols-3 gap-6 sm:mt-16"><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/text-to-sql-llm-app/><img loading=lazy src=/img/blog/2024-06-07-text-to-sql-llm-app/dashboard_hu10486745171311772637.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/text-to-sql-llm-app/ class=block>How to build a text-to-sql LLM application</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">As industry races for use cases of Large Language Models, software devs have emerged as early adopters. Can LLMs help us translate between tech and talk? Let&rsquo;s build a text-to-SQL application with Vanna and Streamlit!</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/team/prema-roman_hu8516555698015783157.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>Jun 7, 2024</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/starting-simple-with-ai/><img loading=lazy src=/img/blog/2024-05-08-to-llm-or-not-to-llm-that-is-the-question-part-2/cover-photo_hu3708469912556123294.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llms>LLMs</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/data>Data</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/starting-simple-with-ai/ class=block>To LLM or Not to LLM (Part 2): Starting Simple</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">Sick of hearing about hyped up AI solutions that sound like hot air? 🧐 Let&rsquo;s use boring old ML to detect hype in AI marketing text and see why starting with a simple ML approach is still your best bet 90% of the time.</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/butterfly_hu14572934016300068338.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/danielle-maxwell>Danielle Maxwell</a>,&nbsp;</li><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>May 20, 2024</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/responsible-innovation/><img loading=lazy src=/img/blog/2024-05-08-to-llm-or-not-to-llm-that-is-the-question-part-1/elena-mozhvilo-unsplash_hu7164420345709980639.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llms>LLMs</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/data>Data</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/responsible-innovation/ class=block>To LLM or Not to LLM: Tips for Responsible Innovation</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">We&rsquo;re seeing a proliferation of Large Language Models (LLMs) as companies seek to replicate OpenAI&rsquo;s success. In this post, two AI engineers respond to LLM FAQs and offer tips for responsible innovation.</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/butterfly_hu14572934016300068338.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/danielle-maxwell>Danielle Maxwell</a>,&nbsp;</li><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>May 8, 2024</div></div></div></li></ul></div></div></div><div class="bg-[#1D65A6] max-w-[800px] mx-auto mt-20 py-14 px-12 md:px-16 text-white md:rounded-lg"><form action=blog method=post id=newsletterForm><h6 class="font-bold text-center">Enter Your Email To Subscribe</h6><label for=email class=hidden>Email</label>
<input type=text name=email id=email required placeholder class="w-full px-4 py-2.5 rounded-lg mt-6 text-black" style=color:#000><div class="flex mt-6 items-start gap-x-2"><input type=checkbox id=checkbox required class="mt-1 w-4 h-4 block border-0">
<label for=checkbox><span>I want to receive the monthly newsletter and other updates from Rotational. You agree to our Privacy Policy. You may unsubscribe at any time.*</span></label></div><div class="bg-teal-100 border-t-4 border-teal-500 mt-10 rounded-b text-teal-900 px-4 py-3 shadow-md hidden" id=newsletter-alert role=alert><div class=flex><div><p class=text-sm>Thank you for your interest!</p></div></div></div><div class="flex justify-center"><button type=submit class="bg-[#192E5B] px-14 py-4 mt-10 rounded-lg text-sm text-white uppercase md:text-base">
Submit</button></div></form></div></main><footer class="relative mt-40 md:mt-56 bg-[#192E5B]"><div class="relative w-full pt-36 md:pt-16 lg:pt-24 2xl:pt-20 font-extralight text-white"><div class="-mt-52 w-full mx-auto max-w-screen-xl px-4"><section class="bg-[#72A2C0] w-full p-6 md:py-20 md:px-16"><h2 class="my-4 text-2xl sm:text-3xl md:text-5xl text-white font-extrabold">LET'S ENVISION & BUILD THE FUTURE TOGETHER.</h2><div class=py-6><a href=/contact class="p-3 md:p-4 md:px-6 bg-[#2F4858] font-bold md:text-lg text-white text-center hover:bg-[#2F4858]/80">CONTACT US</a></div></section></div><div class="max-w-7xl mx-auto px-6"><div class="mt-12 flex flex-col md:flex-row lg:justify-between gap-x-8"><div class="my-4 max-w-xs"><h5 class="mb-3 font-extrabold">OUR PRESENCE</h5><p>We share because we care, about topics, tools, and technologies that we believe impact the AI economy.</p><div class=py-4><ul class="flex justify-between items-center gap-x-8"><li><a href=https://twitter.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-x-twitter"></i><p class=sr-only>Twitter</p></a></li><li><a href=https://www.linkedin.com/company/rotational target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-linkedin"></i><p class=sr-only>LinkedIn</p></a></li><li><a href=https://github.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-github"></i><p class=sr-only>GitHub</p></a></li><li><a href=https://www.youtube.com/@rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-youtube"></i><p class=sr-only>YouTube</p></a></li><li><a href=https://www.twitch.tv/rotationallabs target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-twitch"></i><p class=sr-only>Twitch</p></a></li></ul></div></div><div class=my-4><h5 class="mb-3 font-extrabold">COMPANY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/about>About Us</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/case-studies>Case Studies</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/endeavor>Endeavor</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/blog>Blog</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">COMMUNITY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/learning>Learning</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/opensource>Open Source</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">CONTACT US</h5><ul><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-map-marker-alt text-[#757575]"></i>
St. Paul, MN & Washington, DC</li><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-envelope text-[#757575]"></i>
info@rotational.io</li></ul><div class=py-8><a href=/contact class="p-3 bg-[#ECF6FF] font-bold text-black text-center hover:bg-[#ECF6FF]/80">CONTACT US</a></div></div></div><div class="sm:flex justify-between py-6 border-t mt-4"><p>Copyright © Rotational Labs, Inc. 2021–2025 · All Rights Reserved</p><div><ul class="sm:mt-0 mt-4 flex gap-x-8"><li><a href=/privacy/>Privacy Policy</a></li><li><a href=/terms/>Terms of Use</a></li></ul></div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/flowbite@2.5.2/dist/flowbite.min.js></script><script type=text/javascript id=hs-script-loader async defer src=//js.hs-scripts.com/24168101.js></script><script src=https://rotational.io/js/blogSingle.js></script></body></html>