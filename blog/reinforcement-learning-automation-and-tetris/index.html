<!doctype html><html lang=en-us><head><meta charset=UTF-8><title>Rotational Labs | Reinforcement Learning, Automation... and Tetris</title>
<base href=https://rotational.io/ target=_self><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Rotational Labs, Inc."><meta name=description content="If you're looking for clever ways to automate decision making at work, don't forget about reinforcement learning! RL is much more flexible than manually writing out a ton rules in your code, and it can uncover novel insights that supervised machine learning models cannot."><meta name=keywords content="ai for companies,ai software,ai consulting,ai solutions,ai development,enterprise ai,ai services,ai platform,ai applications,ai for business,artificial intelligence solutions,artificial intelligence consulting"><link type=text/plain rel=author href=https://rotational.io/humans.txt><meta property="og:title" content="Reinforcement Learning, Automation... and Tetris"><meta property="og:description" content="If you're looking for clever ways to automate decision making at work, don't forget about reinforcement learning! RL is much more flexible than manually writing out a ton rules in your code, and it can uncover novel insights that supervised machine learning models cannot."><meta property="og:image" content="https://rotational.io/img/blog/gameboy.jpg"><meta property="og:url" content="https://rotational.io/blog/reinforcement-learning-automation-and-tetris/"><meta property="og:type" content="website"><meta name=twitter:title content="Reinforcement Learning, Automation... and Tetris"><meta name=twitter:card content="summary"><meta name=twitter:description content="If you're looking for clever ways to automate decision making at work, don't forget about reinforcement learning! RL is much more flexible than manually writing out a ton rules in your code, and it can uncover novel insights that supervised machine learning models cannot."><meta name=twitter:image content="https://rotational.io/img/blog/gameboy.jpg"><link rel="shortcut icon" href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=icon href=https://rotational.io/img/favicon.png type=image/x-icon><link rel=alternate type=application/rss+xml href=https://rotational.io//index.xml title="Recent Rotations of the Rotational Labs Blog"><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Montserrat:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" rel=stylesheet><link rel=stylesheet href=https://rotational.io/output.css media=screen><script src="https://www.googletagmanager.com/gtag/js?id=G-2FKX6CWJHW" async defer></script><script type=module>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());
  gtag('config', 'G-2FKX6CWJHW');
</script><script async defer src="https://www.google.com/recaptcha/enterprise.js?render=6Ld5O3kiAAAAAJU0z0h81X1RxEMHyoROe6KWe_vk"></script><script src=https://kit.fontawesome.com/fea17a4e21.js crossorigin=anonymous></script></head><body><div class="relative bg-[#1D65A6]"><nav class="w-full max-w-7xl mx-auto px-4 sm:px-6 py-5" aria-label=Global><div class="relative max-w-7xl w-full flex flex-wrap lg:flex-nowrap items-center justify-between mx-auto"><a href=/><img src=/img/rototational-white-text-only_hu15583614935666962927.webp alt=Rotational class="h-6 w-auto">
</a><button data-collapse-toggle=navbar-dropdown type=button class="inline-flex items-center p-2 w-10 h-10 justify-center text-sm rounded-lg lg:hidden hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200" aria-controls=navbar-dropdown aria-expanded=false>
<span class=sr-only>Open main menu</span>
<i class="fa fa-bars text-xl text-white"></i></button><div class="hidden absolute z-[9999] lg:static top-16 w-[92%] md:w-[96%] lg:w-auto lg:block bg-white lg:bg-transparent text-[#192E5B] lg:text-[#F2F2F2] py-2 rounded-md" id=navbar-dropdown><ul class="flex flex-col lg:gap-2 xl:gap-4 md:flex-row font-semibold w-full md:justify-between"><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/about/>About</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><button id=dropdownNavbarLink data-dropdown-toggle=dropdownNavbar class="uppercase flex items-center justify-between gap-x-2 w-full px-3 rounded md:border-0 md:p-0 md:w-auto hover:text-black">
Services
<i class="fa fa-angle-down pt-1"></i></button><div id=dropdownNavbar class="z-10 hidden font-normal bg-[#192E5B] divide-y divide-[#192E5B] rounded-lg shadow w-44"><ul class="py-2 text-sm text-[#F2F2F2]" aria-labelledby=dropdownLargeButton><li><a href=/services/ai-assessments/ class="block px-3 py-2 hover:font-bold">AI Assessments</a></li><li><a href=/services/ai-product-development/ class="block px-3 py-2 hover:font-bold">AI Product Development</a></li><li><a href=/services/ai-ops-and-data-foundations/ class="block px-3 py-2 hover:font-bold">AI Ops & Data Foundations</a></li></ul></div></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/case-studies>Case Studies</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/blog/>Blog</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/learning/>Learning</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/endeavor/>Product</a></li><li class="py-2 uppercase text-sm lg:text-[15px] xl:text-[17px]"><a class="px-3 py-3.5 hover:text-black" href=/contact/>Contact</a></li></ul></div></div></nav></div><main><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class=mt-14><div class=blog-img><img src=/img/blog/gameboy_hu16563883237312046117.webp alt="Reinforcement Learning, Automation... and Tetris" class="mx-auto object-cover"></div><div class=mt-8><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center" data-blog-title="Reinforcement Learning, Automation... and Tetris"><b class=text-[#1D65A6]>Reinforcement </b>Learning, Automation... and Tetris</h3><div class="flex flex-wrap justify-center items-center my-4"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span><a href=/authors/patrick-deziel>Patrick Deziel</a> | Thursday, Nov 2, 2023 |&nbsp;
</span><span><a href=/tags/reinforcement-learning>Reinforcement Learning</a>,&nbsp;
</span><a href=/tags/ai>AI</a>,&nbsp;
</span><a href=/tags/python>Python</a></span></div><article class="max-w-[800px] mx-auto prose"><p>Every time I&rsquo;m writing complex rules in my code, I remember there&rsquo;s a machine learning technique for this: Reinforcement Learning. RL models are able to learn the optimal rules given predefined rewards. Read on to learn how!</p><h2 id=reinforcement-learning>Reinforcement Learning</h2><p>Reinforcement learning is a branch of machine learning where the goal is to train an intelligent agent to take actions in an environment in order to maximize a &ldquo;reward&rdquo;. RL has famously been used to defeat the best human players in <a href=https://deepmind.google/technologies/alphago/>Go</a> (the board game, not the programming language), but the approach is generic enough to extend to domains beyond gaming (robotics, natural language processing, etc.).</p><p>In this post we will explore creating a RL agent of our own to play Tetris. If you just want the code, you can check out the repo <a href=https://github.com/pdeziel/ai-tetris>here</a>.</p><h2 id=prerequisites>Prerequisites</h2><p>This is a Python project with some dependencies:</p><ul><li><a href=https://gymnasium.farama.org/>gymnasium</a>, a RL framework from OpenAI, the makers of ChatGPT</li><li><a href=https://stable-baselines3.readthedocs.io/en/master/index.html>stable-baselines3</a>, a Python library with several implemented RL algorithms</li><li><a href=https://pypi.org/project/pyensign/>pyensign</a>, the Ensign Python SDK so we can store and retrieve trained models</li><li><a href=https://pypi.org/project/pyboy/>pyboy</a>, a handy GameBoy emulator for Python</li><li><a href=https://pypi.org/project/numpy/>numpy</a> and <a href=https://pypi.org/project/pandas/>pandas</a>, Python ML staples</li><li><a href=https://pypi.org/project/python-ulid/>python-ulid</a>, because sortable IDs are nice</li></ul><h2 id=going-to-the-gym>Going to the Gym</h2><p>To build our agent we will use <a href=https://gymnasium.farama.org/>gymnasium</a>, an open source (MIT License) Python package from the same organization behind ChatGPT.</p><p>Our training loop will look something like this:</p><p><img alt="&ldquo;RL Workflow&rdquo;" src=/img/blog/2023-10-31-reinforcement-learning-model-versioning-and-tetris/workflow.png></p><p>At each step the agent takes an action from the available action space {A, B, Up, Down, Left, Right, Pass} and makes an input to the emulator where the game is running. Then the agent receives a representation of the game state along with the reward for taking the action. The goal is to incentivize the agent to take actions that yield the best cumulative reward over time.</p><p>Note that unlike supervised learning, we don&rsquo;t need any labeled data. In fact, we&rsquo;ll be training the agent to play Tetris without telling it the &ldquo;rules&rdquo; of the game or how to score points!</p><p>We can create a custom <code>TetrisEnv</code> that subclasses <code>gymnasium.Env</code> and implements the following methods: <code>reset()</code>, <code>step()</code>, <code>render()</code>.</p><h3 id=reset>reset</h3><p>Reset the state of the game to the initial state. This is called at the beginning of each &ldquo;episode&rdquo; (e.g. when the board gets filled and it&rsquo;s game over). We also introduce some randomness here to ensure that the agent explores more possible states of the game and it doesn&rsquo;t get accidentally optimize for one scenario.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>reset</span>(self, seed<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    super()<span style=color:#f92672>.</span>reset(seed<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load the initial state</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>init_state <span style=color:#f92672>!=</span> <span style=color:#e6db74>&#34;&#34;</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> open(self<span style=color:#f92672>.</span>init_state, <span style=color:#e6db74>&#34;rb&#34;</span>) <span style=color:#66d9ef>as</span> f:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pyboy<span style=color:#f92672>.</span>load_state(f)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Randomize the game state</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> seed <span style=color:#f92672>is</span> <span style=color:#f92672>not</span> <span style=color:#66d9ef>None</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(seed <span style=color:#f92672>%</span> <span style=color:#ae81ff>60</span>):
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pyboy<span style=color:#f92672>.</span>tick()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    observation <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>current_score <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>get_total_score(observation)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>board <span style=color:#f92672>=</span> observation
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> observation, {}
</span></span></code></pre></div><h3 id=render>render</h3><p>Render defines what the current observable state of the game is. In this method we extract a simplified version of the board as a multi-dimensional <code>numpy</code> array so that the model has something to work with.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>render</span>(self):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Render the sprite map on the backgound</span>
</span></span><span style=display:flex><span>    background <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>asarray(self<span style=color:#f92672>.</span>manager<span style=color:#f92672>.</span>tilemap_background()[<span style=color:#ae81ff>2</span>:<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>0</span>:<span style=color:#ae81ff>18</span>])
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>observation <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(background <span style=color:#f92672>==</span> <span style=color:#ae81ff>47</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Find all tile indexes for the current tetromino</span>
</span></span><span style=display:flex><span>    sprite_indexes <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>manager<span style=color:#f92672>.</span>sprite_by_tile_identifier(self<span style=color:#f92672>.</span>sprite_tiles, on_screen<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> sprite_tiles <span style=color:#f92672>in</span> sprite_indexes:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> sprite_idx <span style=color:#f92672>in</span> sprite_tiles:
</span></span><span style=display:flex><span>            sprite <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>manager<span style=color:#f92672>.</span>sprite(sprite_idx)
</span></span><span style=display:flex><span>            tile_x <span style=color:#f92672>=</span> (sprite<span style=color:#f92672>.</span>x <span style=color:#f92672>//</span> <span style=color:#ae81ff>8</span>) <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>            tile_y <span style=color:#f92672>=</span> sprite<span style=color:#f92672>.</span>y <span style=color:#f92672>//</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> tile_x <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>output_shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>and</span> tile_y <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>output_shape[<span style=color:#ae81ff>0</span>]:
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>observation[tile_y, tile_x] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    logging<span style=color:#f92672>.</span>debug(<span style=color:#e6db74>&#34;Board State:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(self<span style=color:#f92672>.</span>observation))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>observation
</span></span></code></pre></div><h3 id=step>step</h3><p>Step is called to advance the state of the game once step by applying a button press (or sometimes no button press). It needs to return the current state of the board and also the reward for taking the action. To start, we will give out a positive reward for an action that increases the in-game score and a massive negative reward for reaching the &ldquo;Game Over&rdquo; state.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>step</span>(self, action):
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>do_input(self<span style=color:#f92672>.</span>valid_actions[action])
</span></span><span style=display:flex><span>    observation <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> observation[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>sum() <span style=color:#f92672>&gt;=</span> len(observation[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Game over</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> observation, <span style=color:#f92672>-</span><span style=color:#ae81ff>100</span>, <span style=color:#66d9ef>True</span>, <span style=color:#66d9ef>False</span>, {}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Set reward equal to difference between current and previous score</span>
</span></span><span style=display:flex><span>    total_score <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>get_total_score(observation)
</span></span><span style=display:flex><span>    reward <span style=color:#f92672>=</span> total_score <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>current_score
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>current_score <span style=color:#f92672>=</span> total_score
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>board <span style=color:#f92672>=</span> observation
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    logging<span style=color:#f92672>.</span>debug(<span style=color:#e6db74>&#34;Total Score: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(total_score))
</span></span><span style=display:flex><span>    logging<span style=color:#f92672>.</span>debug(<span style=color:#e6db74>&#34;Reward: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(reward))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> observation, reward, <span style=color:#66d9ef>False</span>, <span style=color:#66d9ef>False</span>, {}
</span></span></code></pre></div><h2 id=tracking-training-runs>Tracking Training Runs</h2><p>When training machine learning models it&rsquo;s often necessary to keep tabs on how the training is progressing. The <code>stable_baselines3</code> library has an interface for creating custom log writers, so why don&rsquo;t we create an <code>Ensign</code> writer? When the <code>write</code> method is called, it will publish an <code>Event</code> to an Ensign topic, similar to writing a log event to a file on disk.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> json
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> logging
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.events <span style=color:#f92672>import</span> Event
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> stable_baselines3.common.logger <span style=color:#f92672>import</span> KVWriter, filter_excluded_keys
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>EnsignWriter</span>(KVWriter):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    EnsignWriter subclasses the Stable Baselines3 KVWriter class to write key-value pairs to Ensign.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, ensign, topic<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;agent-training&#34;</span>, agent_id<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>        super()<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ensign <span style=color:#f92672>=</span> ensign
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>topic <span style=color:#f92672>=</span> topic
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>version <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;0.1.0&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>agent_id <span style=color:#f92672>=</span> agent_id
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>publish</span>(self, event):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        One-off publish to Ensign.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(self<span style=color:#f92672>.</span>topic, event)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>flush()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span> asyncio<span style=color:#f92672>.</span>TimeoutError:
</span></span><span style=display:flex><span>            logging<span style=color:#f92672>.</span>warning(<span style=color:#e6db74>&#34;Timeout exceeded while flushing Ensign writer.&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>write</span>(self, key_values, key_excluded, step<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Write the key-value pairs to Ensign.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        meta <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#34;step&#34;</span>: step}
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>agent_id:
</span></span><span style=display:flex><span>            meta[<span style=color:#e6db74>&#34;agent_id&#34;</span>] <span style=color:#f92672>=</span> str(self<span style=color:#f92672>.</span>agent_id)
</span></span><span style=display:flex><span>        key_values <span style=color:#f92672>=</span> filter_excluded_keys(key_values, key_excluded, <span style=color:#e6db74>&#34;ensign&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> key, value <span style=color:#f92672>in</span> key_values<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>            <span style=color:#75715e># JSON doesn&#39;t support numpy types</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> isinstance(value, np<span style=color:#f92672>.</span>float32):
</span></span><span style=display:flex><span>                key_values[key] <span style=color:#f92672>=</span> float(value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        event <span style=color:#f92672>=</span> Event(json<span style=color:#f92672>.</span>dumps(key_values)<span style=color:#f92672>.</span>encode(<span style=color:#e6db74>&#34;utf-8&#34;</span>), mimetype<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;application/json&#34;</span>, schema_name<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;training_log&#34;</span>, schema_version<span style=color:#f92672>=</span>self<span style=color:#f92672>.</span>version, meta<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;agent_id&#34;</span>: str(self<span style=color:#f92672>.</span>agent_id), <span style=color:#e6db74>&#34;step_number&#34;</span>: str(step)})
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Invoke publish in the appropriate event loop</span>
</span></span><span style=display:flex><span>        publish <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>publish(event)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            loop <span style=color:#f92672>=</span> asyncio<span style=color:#f92672>.</span>get_event_loop()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span> <span style=color:#a6e22e>RuntimeError</span>:
</span></span><span style=display:flex><span>            asyncio<span style=color:#f92672>.</span>run(publish)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>run_coroutine_threadsafe(publish, loop)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>close</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Close the Ensign writer.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        asyncio<span style=color:#f92672>.</span>run(self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>close())
</span></span></code></pre></div><p>The cool part is that this lets us inject our own metadata tags which we can query on later. In the Ensign topic dashboard, we will be able to lookup previous training sessions to quantify training progress.</p><p><img alt="&ldquo;Agent Sessions&rdquo;" src=/img/blog/2023-10-31-reinforcement-learning-model-versioning-and-tetris/agent-session.png></p><h2 id=starting-the-training>Starting the Training</h2><p>Now that we have a RL environment setup, it&rsquo;s time to train some models! Since we will probably be experimenting with different types of models is usually helpful to create a generic &ldquo;trainer&rdquo; class.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> io
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> os
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> datetime <span style=color:#f92672>import</span> datetime
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ulid <span style=color:#f92672>import</span> ULID
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> stable_baselines3
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.events <span style=color:#f92672>import</span> Event
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>AgentTrainer</span>:
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    AgentTrainer can train and evaluate an agent for a reinforcement learning task.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, ensign<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, model_topic<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;agent-models&#34;</span>, model_dir<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>, agent_id<span style=color:#f92672>=</span>ULID()):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>ensign <span style=color:#f92672>=</span> ensign
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_topic <span style=color:#f92672>=</span> model_topic
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>agent_id <span style=color:#f92672>=</span> agent_id
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>model_dir <span style=color:#f92672>=</span> model_dir
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(self, model, sessions<span style=color:#f92672>=</span><span style=color:#ae81ff>40</span>, runs_per_session<span style=color:#f92672>=</span><span style=color:#ae81ff>4</span>, model_version<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;0.1.0&#34;</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Train the agent for the specified number of steps.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        model_name <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__
</span></span><span style=display:flex><span>        policy_name <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>policy<span style=color:#f92672>.</span>__class__<span style=color:#f92672>.</span>__name__
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>ensign:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>ensure_topic_exists(self<span style=color:#f92672>.</span>model_topic)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>model_dir:
</span></span><span style=display:flex><span>            os<span style=color:#f92672>.</span>makedirs(self<span style=color:#f92672>.</span>model_dir, exist_ok<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Train for the number of sessions</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(sessions):
</span></span><span style=display:flex><span>            session_start <span style=color:#f92672>=</span> datetime<span style=color:#f92672>.</span>now()
</span></span><span style=display:flex><span>            model<span style=color:#f92672>.</span>learn(total_timesteps<span style=color:#f92672>=</span>model<span style=color:#f92672>.</span>n_steps <span style=color:#f92672>*</span> runs_per_session)
</span></span><span style=display:flex><span>            session_end <span style=color:#f92672>=</span> datetime<span style=color:#f92672>.</span>now()
</span></span><span style=display:flex><span>            duration <span style=color:#f92672>=</span> session_end <span style=color:#f92672>-</span> session_start
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Ensure that async loggers have a chance to run</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> asyncio<span style=color:#f92672>.</span>sleep(<span style=color:#ae81ff>5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#75715e># Save the model</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>ensign:
</span></span><span style=display:flex><span>                buffer <span style=color:#f92672>=</span> io<span style=color:#f92672>.</span>BytesIO()
</span></span><span style=display:flex><span>                model<span style=color:#f92672>.</span>save(buffer)
</span></span><span style=display:flex><span>                model_event <span style=color:#f92672>=</span> Event(buffer<span style=color:#f92672>.</span>getvalue(), <span style=color:#e6db74>&#34;application/octet-stream&#34;</span>, schema_name<span style=color:#f92672>=</span>model_name, schema_version<span style=color:#f92672>=</span>model_version, meta<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;agent_id&#34;</span>: str(self<span style=color:#f92672>.</span>agent_id), <span style=color:#e6db74>&#34;model&#34;</span>: model_name, <span style=color:#e6db74>&#34;policy&#34;</span>: policy_name, <span style=color:#e6db74>&#34;trained_at&#34;</span>: session_end<span style=color:#f92672>.</span>isoformat(), <span style=color:#e6db74>&#34;train_seconds&#34;</span>: str(duration<span style=color:#f92672>.</span>total_seconds())})
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>publish(self<span style=color:#f92672>.</span>model_topic, model_event)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>model_dir:
</span></span><span style=display:flex><span>                model<span style=color:#f92672>.</span>save(os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(self<span style=color:#f92672>.</span>model_dir, <span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>_</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>.zip&#34;</span><span style=color:#f92672>.</span>format(model_name, session_end<span style=color:#f92672>.</span>strftime(<span style=color:#e6db74>&#34;%Y%m</span><span style=color:#e6db74>%d</span><span style=color:#e6db74>-%H%M%S&#34;</span>))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>ensign:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>await</span> self<span style=color:#f92672>.</span>ensign<span style=color:#f92672>.</span>flush()
</span></span></code></pre></div><p>Storing the models in Ensign gives us a convenient way to checkpoint the model at the end of each session. Note that we are including the model name and version as part of the event schema which allows us to keep track of which models we&rsquo;ve trained.</p><p><img alt="&ldquo;Agent Models&rdquo;" src=/img/blog/2023-10-31-reinforcement-learning-model-versioning-and-tetris/agent-models.png></p><p>At this point we can kick off the training with a PPO model from the <code>stable_baselines3</code> library.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> asyncio
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> argparse
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> ulid <span style=color:#f92672>import</span> ULID
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> stable_baselines3 <span style=color:#f92672>import</span> PPO
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> pyensign.ensign <span style=color:#f92672>import</span> Ensign
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> stable_baselines3.common.logger <span style=color:#f92672>import</span> Logger, make_output_format
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> agent <span style=color:#f92672>import</span> AgentTrainer
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> writer <span style=color:#f92672>import</span> EnsignWriter
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tetris_env <span style=color:#f92672>import</span> TetrisEnv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>parse_args</span>():
</span></span><span style=display:flex><span>    <span style=color:#75715e># Parse the command line arguments</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>async</span> <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(args):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Create Ensign client</span>
</span></span><span style=display:flex><span>    ensign <span style=color:#f92672>=</span> Ensign(cred_path<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>creds)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Configure the environment, model, and trainer</span>
</span></span><span style=display:flex><span>    agent_id <span style=color:#f92672>=</span> ULID()
</span></span><span style=display:flex><span>    env <span style=color:#f92672>=</span> TetrisEnv(gb_path<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>rom, action_freq<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>freq, speedup<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>speedup, init_state<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>init, log_level<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>log_level, window<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;headless&#34;</span>)
</span></span><span style=display:flex><span>    trainer <span style=color:#f92672>=</span> AgentTrainer(ensign<span style=color:#f92672>=</span>ensign, model_topic<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>model_topic, agent_id<span style=color:#f92672>=</span>agent_id)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> PPO(args<span style=color:#f92672>.</span>policy, env, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, n_steps<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>steps, batch_size<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>batch_size, n_epochs<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>epochs, gamma<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>gamma)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Set logging outputs</span>
</span></span><span style=display:flex><span>    output_formats <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> args<span style=color:#f92672>.</span>log_stdout:
</span></span><span style=display:flex><span>        output_formats<span style=color:#f92672>.</span>append(make_output_format(<span style=color:#e6db74>&#34;stdout&#34;</span>, <span style=color:#e6db74>&#34;sessions&#34;</span>))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> args<span style=color:#f92672>.</span>session_topic:
</span></span><span style=display:flex><span>        writer <span style=color:#f92672>=</span> EnsignWriter(ensign, topic<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>session_topic, agent_id<span style=color:#f92672>=</span>agent_id)
</span></span><span style=display:flex><span>        output_formats<span style=color:#f92672>.</span>append(writer)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>set_logger(Logger(<span style=color:#66d9ef>None</span>, output_formats<span style=color:#f92672>=</span>output_formats))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>await</span> trainer<span style=color:#f92672>.</span>train(model, sessions<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>sessions, runs_per_session<span style=color:#f92672>=</span>args<span style=color:#f92672>.</span>runs)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>await</span> ensign<span style=color:#f92672>.</span>close()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#34;__main__&#34;</span>:
</span></span><span style=display:flex><span>    asyncio<span style=color:#f92672>.</span>run(train(parse_args()))
</span></span></code></pre></div><h2 id=evaluation>Evaluation</h2><p>Now that all our models are in Ensign, it&rsquo;s possible to evaluate any iteration of the PP0 model against a baseline. For example, if we want to evaluate the latest PP0 model we can use a pyensign query to retrieve it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Get the most recent model for the schema</span>
</span></span><span style=display:flex><span>model_events <span style=color:#f92672>=</span> <span style=color:#66d9ef>await</span> ensign<span style=color:#f92672>.</span>query(<span style=color:#e6db74>&#34;SELECT * FROM agent-models.PP0&#34;</span>)<span style=color:#f92672>.</span>fetchall()
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model_events[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Deserialize the model from the event</span>
</span></span><span style=display:flex><span>buffer <span style=color:#f92672>=</span> io<span style=color:#f92672>.</span>BytesIO(model<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>model_class <span style=color:#f92672>=</span> getattr(stable_baselines3, model<span style=color:#f92672>.</span>meta[<span style=color:#e6db74>&#34;model&#34;</span>])
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> model_class<span style=color:#f92672>.</span>load(buffer)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Run the model in the environment</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(runs):
</span></span><span style=display:flex><span>    seed <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>100000</span>)
</span></span><span style=display:flex><span>    obs, _ <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>reset(seed<span style=color:#f92672>=</span>seed)
</span></span><span style=display:flex><span>    terminated <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    steps <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> <span style=color:#f92672>not</span> terminated:
</span></span><span style=display:flex><span>        action, _states <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(obs, deterministic<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        obs, reward, terminated, _, _ <span style=color:#f92672>=</span> env<span style=color:#f92672>.</span>step(action)
</span></span><span style=display:flex><span>        env<span style=color:#f92672>.</span>render()
</span></span><span style=display:flex><span>        steps <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{}</span><span style=color:#e6db74>: Seed: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>, Steps: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>, Score: </span><span style=color:#e6db74>{}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(schema, seed, steps, env<span style=color:#f92672>.</span>get_score()))
</span></span></code></pre></div><p>The random baseline model never manages to score points. After training the PPO model for ~1.6 million steps, it actually starts to complete some lines. Also, on average it survives longer than the random model. I think we can declare victory!</p><table><thead><tr><th style=text-align:center>Baseline</th><th style=text-align:center>PPO Model</th></tr></thead><tbody><tr><td style=text-align:center><div class="items-center my-3"><img src=https://rotational.io/img/blog/2023-10-31-reinforcement-learning-model-versioning-and-tetris/baseline.gif alt class="m-auto h-[300px]"></div></td><td style=text-align:center><div class="items-center my-3"><img src=https://rotational.io/img/blog/2023-10-31-reinforcement-learning-model-versioning-and-tetris/pp0.gif alt class="m-auto h-[300px]"></div></td></tr></tbody></table><pre tabindex=0><code>Using model PPO v0.1.0
Baseline: Seed: 21523, Steps: 486, Score: 0
PPO v0.1.0: Seed: 21523, Steps: 518, Score: 0

Baseline: Seed: 96145, Steps: 328, Score: 0
PPO v0.1.0: Seed: 96145, Steps: 496, Score: 0

Baseline: Seed: 96558, Steps: 315, Score: 0
PPO v0.1.0: Seed: 96558, Steps: 532, Score: 0

Baseline: Seed: 80284, Steps: 308, Score: 0
PPO v0.1.0: Seed: 80284, Steps: 554, Score: 64

Baseline: Seed: 40057, Steps: 374, Score: 0
PPO v0.1.0: Seed: 40057, Steps: 524, Score: 64

Baseline: Seed: 80484, Steps: 486, Score: 0
PPO v0.1.0: Seed: 80484, Steps: 589, Score: 64
</code></pre><h2 id=what-next>What Next?</h2><p>So, if you&rsquo;re looking for clever ways to automate decision making at work, don&rsquo;t forget about reinforcement learning! Just like Tetris, it might take a bit of practice to get the hang of. But once you&rsquo;ve added RL to your toolkit, you&rsquo;ll see that it&rsquo;s a much more flexible (and often less bug-prone) method compared to manually writing out a ton rules in your code. Even better, RL can uncover novel insights that supervised machine learning models cannot, thanks to tricks like the <a href=https://en.wikipedia.org/wiki/Multi-armed_bandit>explore/exploit algorithm</a>.</p><p>The best way to get good at RL? Get some practice objectively defining your reward (or punishment), and experiment with different policies and algorithms. Using the RL framework in our Tetris example, you should be able to do that a lot more quickly!</p><p>Here are some things you could try next:</p><ul><li>Experiment with different RL models and policies</li><li>Create a more nuanced reward function that rewards shorter-term actions (e.g. minimize the number of &ldquo;holes&rdquo; in the board)</li><li>Parallelize the training to explore the decision space</li></ul><p>Note that since each agent has a unique ID, training multiple agents at once is not a problem since we can always query on the agent ID in the metadata tags.</p><h2 id=credits>Credits</h2><p>This post is heavily inspired by the repo <a href=https://github.com/PWhiddy/PokemonRedExperiments/tree/master>here</a> which tackles a much more complicated game environment!</p><div class="border-t my-12"></div><p>Photo by Ravi Palwe on Unsplash</p></article></div></div><div class="bg-[#E8EFF6] max-w-[800px] mx-auto mt-9 p-8 rounded-lg"><h3 class="font-bold text-xl sm:text-2xl lg:text-3xl text-center mb-3"><span class="text-[#1D65A6] font-bold">About </span>This Post</h3><p class="text-base mx-auto px-2 text-center lg:text-base">If you're looking for clever ways to automate decision making at work, don't forget about reinforcement learning! RL is much more flexible than manually writing out a ton rules in your code, and it can uncover novel insights that supervised machine learning models cannot.</p><div class="flex flex-col md:flex-row text-center mx-auto border-t pt-6 mt-6 align-center justify-between gap-10"><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Written by:</h2><div class="flex items-center"><a href=/authors/patrick-deziel><img src=img/team/patrick-deziel.png alt class="mr-3 border-4 border-white rounded-full h-11 drop-shadow-lg">
</a><span class="flex flex-wrap"><a href=/authors/patrick-deziel class="lg:w-[20ch] mx-2">Patrick Deziel</a></span></div></div><div class=lg:w-1/2><h2 class="text-lg text-[#1D65A6] font-bold mb-3">Share this post:</h2><ul class="flex items-center justify-center gap-6 mt-4"><li><a onclick=shareByEmail() class=cursor-pointer><img src=img/email.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnTwitterWithTitle() class=cursor-pointer><img src=img/twitter.png alt class="rounded-lg bg-white p-3"></a></li><li><a onclick=shareOnLinkedIn() class=cursor-pointer><img src=img/linkedin.png alt class="rounded-lg bg-white p-3"></a></li></ul></div></div></div><div class="relative max-w-7xl mx-auto px-4 sm:px-6"><div class="flex justify-between mt-12 sm:mt-24 items-center"><div class="flex items-center"><h2 class="font-bold text-2xl sm:text-4xl flex"><span class=text-[#1D65A6]>Recommended</span>
&nbsp;Rotations</h2></div><div><a href=/blog class="flex text-base sm:text-lg items-center font-bold text-[#1D65A6]"><span>View all</span>
<img src=img/arr-right.png alt class="h-4 ml-2"></a></div></div><div><ul class="grid sm:grid-cols-2 lg:grid-cols-3 gap-6 sm:my-8"><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/text-to-sql-llm-app/><img loading=lazy src=/img/blog/2024-06-07-text-to-sql-llm-app/dashboard_hu10486745171311772637.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llm>LLM</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/text-to-sql-llm-app/ class=block>How to build a text-to-sql LLM application</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">As industry races for use cases of Large Language Models, software devs have emerged as early adopters. Can LLMs help us translate between tech and talk? Let&rsquo;s build a text-to-SQL application with Vanna and Streamlit!</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/team/prema-roman_hu8516555698015783157.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>Jun 7, 2024</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/starting-simple-with-ai/><img loading=lazy src=/img/blog/2024-05-08-to-llm-or-not-to-llm-that-is-the-question-part-2/cover-photo_hu3708469912556123294.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/llms>LLMs</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ml>ML</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/data>Data</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/starting-simple-with-ai/ class=block>To LLM or Not to LLM (Part 2): Starting Simple</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">Sick of hearing about hyped up AI solutions that sound like hot air? 🧐 Let&rsquo;s use boring old ML to detect hype in AI marketing text and see why starting with a simple ML approach is still your best bet 90% of the time.</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/butterfly_hu14572934016300068338.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/danielle-maxwell>Danielle Maxwell</a>,&nbsp;</li><li class=font-extralight><a href=/authors/prema-roman>Prema Roman</a></li></ul></div><div class=font-extralight>May 20, 2024</div></div></div></li><li class="mt-6 bg-[#ECF6FF] rounded-xl"><div class="flex flex-col h-full"><a href=https://rotational.io/blog/building-an-ai-text-detector/><img loading=lazy src=/img/blog/circuit_brain_hu6142668315322609809.webp alt class="rounded-t-xl object-cover" style=height:212px;width:100%></a><div class="px-4 pt-4"><ul class="flex flex-wrap"><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/ai>AI</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/python>Python</a>,&nbsp;</li><li class="text-base font-bold text-[#1D65A6]"><a href=/tags/text-generation>Text Generation</a></li></ul><div class="flex flex-col mt-4 h-full"><h3 class="text-xl font-extrabold sm:h-36"><a href=https://rotational.io/blog/building-an-ai-text-detector/ class=block>Building an AI Text Detector - Lessons Learned</a></h3><div class=mb-4><p class="my-4 sm:mt-auto">The LLMs boom has made differentiating text written by a person vs. generated by AI a highly desired technology. In this post, I&rsquo;ll attempt to build an AI text detector from scratch!</p></div></div></div><div class="flex justify-between mt-auto items-center border-t px-4 py-3 h-16"><div class="flex items-center"><img loading=lazy src=/img/team/patrick-deziel_hu8906543757154920962.png alt class="rounded-full h-10 w-10"><ul class="flex flex-wrap ml-4"><li class=font-extralight><a href=/authors/patrick-deziel>Patrick Deziel</a></li></ul></div><div class=font-extralight>May 15, 2024</div></div></div></li></ul></div></div></div><div class="bg-[#1D65A6] max-w-[800px] mx-auto mt-20 py-14 px-12 md:px-16 text-white md:rounded-lg"><input type=hidden id=newsletterFormID value=8aeeb77b-a5e0-4772-b726-44e1fc2eb6e1><form action=blog method=post id=newsletterForm><h6 class="font-bold text-center">Enter Your Email To Subscribe</h6><label for=email class=hidden>Email</label>
<input type=text name=email id=email required placeholder class="w-full px-4 py-2.5 rounded-lg mt-6 text-black" style=color:#000><div class="flex mt-6 items-start gap-x-2"><input type=checkbox id=checkbox required name=consent class="mt-1 w-4 h-4 block border-0">
<label for=checkbox><span id=consentText>I want to receive the monthly newsletter and other updates from Rotational. You agree to our Privacy Policy. You may unsubscribe at any time.*</span></label></div><div class="bg-teal-100 border-t-4 border-teal-500 mt-10 rounded-b text-teal-900 px-4 py-3 shadow-md hidden" id=newsletter-alert role=alert><div class=flex><div><p class=text-sm>Thank you for your interest!</p></div></div></div><div class="flex justify-center"><button type=submit class="bg-[#192E5B] px-14 py-4 mt-10 rounded-lg text-sm text-white uppercase md:text-base">
Submit</button></div></form></div></main><footer class="relative mt-40 md:mt-56 bg-[#192E5B]"><div class="relative w-full pt-36 md:pt-16 lg:pt-24 2xl:pt-20 font-extralight text-white"><div class="-mt-52 w-full mx-auto max-w-screen-xl px-4"><section class="bg-[#72A2C0] w-full p-6 md:py-20 md:px-16"><h2 class="my-4 text-2xl sm:text-3xl md:text-5xl text-white font-extrabold">LET'S ENVISION & BUILD THE FUTURE TOGETHER.</h2><div class=py-6><a href=/contact class="p-3 md:p-4 md:px-6 bg-[#2F4858] font-bold md:text-lg text-white text-center hover:bg-[#2F4858]/80">CONTACT US</a></div></section></div><div class="max-w-7xl mx-auto px-6"><div class="mt-12 flex flex-col md:flex-row lg:justify-between gap-x-8"><div class="my-4 max-w-xs"><h5 class="mb-3 font-extrabold">OUR PRESENCE</h5><p>We share because we care, about topics, tools, and technologies that we believe impact the AI economy.</p><div class=py-4><ul class="flex justify-between items-center gap-x-8"><li><a href=https://twitter.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-x-twitter"></i><p class=sr-only>Twitter</p></a></li><li><a href=https://www.linkedin.com/company/rotational target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-linkedin"></i><p class=sr-only>LinkedIn</p></a></li><li><a href=https://github.com/rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-github"></i><p class=sr-only>GitHub</p></a></li><li><a href=https://www.youtube.com/@rotationalio target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-youtube"></i><p class=sr-only>YouTube</p></a></li><li><a href=https://www.twitch.tv/rotationallabs target=_blank class=hover:text-[#1D65A6]><i class="text-2xl fa-brands fa-twitch"></i><p class=sr-only>Twitch</p></a></li></ul></div></div><div class=my-4><h5 class="mb-3 font-extrabold">COMPANY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/about>About Us</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/case-studies>Case Studies</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/endeavor>Endeavor</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/blog>Blog</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">COMMUNITY</h5><ul><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/learning>Learning</a></li><li class="pb-3 flex items-center gap-x-2"><i class="fa-solid fa-chevron-right text-[#757575] text-xs"></i>
<a href=/opensource>Open Source</a></li></ul></div><div class=my-4><h5 class="mb-3 font-extrabold">CONTACT US</h5><ul><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-map-marker-alt text-[#757575]"></i>
St. Paul, MN & Washington, DC</li><li class="flex items-baseline lg:items-center gap-x-2"><i class="fa-solid fa-envelope text-[#757575]"></i>
info@rotational.io</li></ul><div class=py-8><a href=/contact class="p-3 bg-[#ECF6FF] font-bold text-black text-center hover:bg-[#ECF6FF]/80">CONTACT US</a></div></div></div><div class="sm:flex justify-between py-6 border-t mt-4"><p>Copyright © Rotational Labs, Inc. 2021–2025 · All Rights Reserved</p><div><ul class="sm:mt-0 mt-4 flex gap-x-8"><li><a href=/privacy/>Privacy Policy</a></li><li><a href=/terms/>Terms of Use</a></li></ul></div></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/flowbite@2.5.2/dist/flowbite.min.js></script><script type=text/javascript id=hs-script-loader async defer src=//js.hs-scripts.com/24168101.js></script><script src=https://rotational.io/js/blogSingle.js></script><script src=https://rotational.io/js/newsletterForm.js></script></body></html>