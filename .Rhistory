"Correlation: ", round(Correlation, 3),
"\nP-val: ", round(pValue, 3)))) +
geom_tile(data=mlt_df) +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1),
name=paste0(corr_method,"\nCorrelation"))
# gheat <- gheat +
#   geom_text(data=mlt_df, aes(Var1, Var2, label = round(Correlation, 2)), size = 2)
gx <- gheat +
geom_point(data=mlt_df_x, shape=4, size=1, stroke=0.5, fill=NA, color="black") +
scale_shape_identity() +
theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=1, size = 7),
axis.text.y = element_text(size = 7),
axis.title.x = element_blank(),
axis.title.y = element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
panel.background = element_blank(),
axis.ticks = element_blank(),
legend.title = element_text(size = 7),
legend.text = element_text(size = 6))
gxly <- ggplotly(gx, tooltip=c('text'))
widgetframe::frameWidget(gxly)
toc()
var_to_remove <- c("reviews_per_month", "availability_60", "availability_90",
"availability_365")
listing_prep2 <- listing_prep2 %>%
select(-all_of(var_to_remove))
set.seed(1234)
training_prop <- 0.8
listing_split <- listing_prep2 %>%
rename(target_var = targetVar) %>%
drop_na(target_var) %>% #remove rows where target variable is missing
mutate(id = row_number()) %>%
initial_split(prop = training_prop, strata = target_var)
listing_train <- training(listing_split)
listing_test <- testing(listing_split)
train_grp <- listing_train %>%
mutate(split = "training")
test_grp <- listing_test %>%
mutate(split = "test")
trainTest_grp <- rbind(train_grp, test_grp)
trainTest_p <- trainTest_grp %>%
rename(!!targetVar := target_var) %>%
select(where(is.numeric) | split) %>%
select(-id) %>%
gather(key, value, -split) %>%
ggplot(aes(x = value, fill = split)) +
facet_wrap(~ key, scales = "free", ncol = 3) +
geom_density(alpha=0.5)
trainTest_ply <- ggplotly(trainTest_p)
trainTest_p2 <- trainTest_grp %>%
select(where(negate(is.numeric)) | split) %>%
gather(key, value, -split) %>%
filter(!is.na(value)) %>%
ggplot(aes(x = value, fill = split)) +
facet_wrap(~ key, scales = "free", ncol = 3) +
geom_bar() +
theme(axis.text.x = element_blank())
p2ly <- ggplotly(trainTest_p2)
rf_res1 <- ranger::ranger(target_var ~ ., data = listing_train %>% na.omit(),
importance = "permutation")
rf_p <- ranger::importance(rf_res1) %>%
enframe("Variable", "Importance") %>%
mutate(Variable = fct_reorder(Variable, Importance)) %>%
arrange(desc(Importance)) %>%
ggplot(aes(x = Variable, y = Importance,
text = paste0(Variable, "\nImportance:", round(Importance,3)))) +
geom_col() +
coord_flip() +
scale_fill_viridis_d(end = .7) +
labs(title = "Feature Importance")
rf_ply <- ggplotly(rf_p, tooltip = c('text'))
widgetframe::frameWidget(rf_ply)
boruta_output <- Boruta(target_var ~ .,
data = listing_train %>% na.omit(),
maxRuns = 50,
doTrace = 0)
boruta_output_tbl <- as.data.frame(boruta_output$ImpHistory) %>%
gather()
fac <- with(boruta_output_tbl, reorder(key, value, median, order = TRUE))
boruta_output_tbl$key <- factor(boruta_output_tbl$key, levels = levels(fac))
boruta_p <- boruta_output_tbl %>%
ggplot(aes(x = key, y = value)) +
geom_boxplot() +
ggtitle("Feature Importance") +
xlab("Variable") +
ylab("Importance") +
coord_flip()
boruta_ply <- ggplotly(boruta_p)
widgetframe::frameWidget(boruta_ply)
selected_var <- c("accommodates", "room_type", "bedrooms", "bathroom_type",
"days_joined", "beds", "amenities_count",
"property_type", "latitude", "longitude", "bathroom",
"price")
set.seed(1234)
training_prop <- 0.8
listing_split <- listing_prep2 %>%
select(all_of(selected_var)) %>%
rename(target_var = targetVar) %>%
na.omit() %>% #remove all rows with missing values
mutate(id = row_number()) %>%
initial_split(prop = training_prop, strata = target_var)
listing_train <- training(listing_split)
listing_test <- testing(listing_split)
#recipe for linear regression
rcpTransform1 <- recipe(target_var ~ ., data = listing_train) %>%
update_role(id, new_role = "id variable") %>%
step_naomit(all_predictors(), skip = TRUE) %>% #remove rows with NA
step_corr(all_numeric(), -all_outcomes(), -id, threshold = 0.7,
method = "pearson") %>% #remove correlated variables at 0.7 threshold
step_normalize(all_numeric(), -all_outcomes(), -id) %>%
step_other(all_predictors(), -all_numeric(),
threshold = 0.05, other = "Others") %>%
step_dummy(all_nominal(), -all_outcomes())
#recipe for decision tree
rcpTransform2 <- recipe(target_var ~ ., data = listing_train) %>%
update_role(id, new_role = "id variable") %>%
step_other(all_predictors(), -all_numeric(),
threshold = 0.05, other = "Others")
#recipe for random forest
rcpTransform3 <- recipe(target_var ~ ., data = listing_train) %>%
update_role(id, new_role = "id variable") %>%
step_other(all_predictors(), -all_numeric(),
threshold = 0.05, other = "Others")
#apply recipe to training data
listingTrain_T <- rcpTransform1 %>% prep() %>% juice()
#get numeric variables only
intersect_vars <- intersect(names(listing_train), names(listingTrain_T))
#assign label for original and processed data
train_proc <- listing_train %>%
select(all_of(intersect_vars)) %>%
mutate(processed = "original")
train_T_proc <- listingTrain_T %>%
select(all_of(intersect_vars)) %>%
mutate(processed = "processed")
trainTProcessed <- rbind(train_proc, train_T_proc)
#plot
trainTProcessed %>%
select(where(is.numeric) | processed) %>%
select(c(1,ncol(.))) %>%
gather(key, value, -processed) %>%
ggplot(aes(x = value, fill = processed)) +
facet_wrap(~ processed, scales = "free") +
geom_histogram(alpha=0.5)
#Linear regression
lm_mod <- linear_reg() %>%
set_engine("lm") %>%
set_mode("regression")
listing_wflow <-  workflow() %>%
add_model(lm_mod) %>%
add_recipe(rcpTransform1)
listing_fit <- listing_wflow %>%
fit(data = listing_train)
glance(listing_fit)
fitResult <- listing_fit %>%
pull_workflow_fit() %>%
tidy()
ggcoefstats(listing_fit$fit$fit$fit,
sort = "ascending",
exclude.intercept = TRUE,
only.significant = TRUE)
#select p-value threshold
pval <- 0.05
#choose sorting by p.value or estimate
sort_by <- "p.value_neg"
# sort_by <- "estimate"
#get significant features
sig_lm_predictors <- fitResult %>%
filter(p.value <= pval) %>%
mutate(p.value_neg = -p.value)
#plot
regly <- ggplotly(
sig_lm_predictors %>%
ggplot(aes(x = estimate, y = reorder(term, !!as.symbol(sort_by)),
text = paste0(term, "\nestimate: ", round(estimate,3),
"\n95% CI: ", round(estimate-std.error,2),
"-", round(estimate+std.error,2),
"\np-value: ", round(p.value,5)))) +
geom_pointrange(aes(xmin = estimate-std.error, xmax = estimate+std.error)) +
geom_vline(xintercept = 0, linetype = "dashed") +
theme(axis.title.y = element_blank()),
tooltip = c('text')
)
widgetframe::frameWidget(regly)
listing_test_T <- rcpTransform1 %>% prep() %>% bake(listing_test)
listing_pred <- predict(listing_fit, new_data = listing_test) %>%
bind_cols(listing_test_T)
multi_metric <- metric_set(mae, mape, rmse, rsq)
final_metric_lm <- listing_pred %>%
multi_metric(truth = target_var, estimate = .pred)
pred_p <- ggplot(listing_pred, aes(x = target_var, y = .pred)) +
geom_point() +
geom_abline() +
coord_obs_pred() +
labs(title = "R-squared plot",
x = paste0("Actual ", targetVar),
y = paste0("Predicted ", targetVar))
pred_ply <- ggplotly(pred_p)
widgetframe::frameWidget(pred_ply)
final_metric_lm
slp <- sig_lm_predictors %>%
filter(term != "(Intercept)") %>%
arrange(p.value) %>%
top_n(9) %>%
pull(term)
slp_train <- listingTrain_T %>%
select(all_of(slp)) %>%
gather()
topn_val <- 10
top_err_id <- listing_pred %>%
mutate(pred_error = abs(target_var-.pred)) %>%
arrange(-pred_error) %>%
top_n(topn_val) %>%
pull(id)
top_err <- listing_pred %>%
filter(id %in% top_err_id) %>%
mutate(pred_error = abs(target_var-.pred)) %>%
select(all_of(slp), id, pred_error) %>%
gather(key, value, -id, -pred_error) %>%
mutate(id = as.factor(id))
color_vec <- rep(c("red"), times = length(unique(top_err$id)))
pred_erly <- ggplotly(ggplot(NULL, aes(x = value)) +
geom_histogram(data = slp_train, color = "grey70",
fill = "grey60", alpha = 0.5) +
geom_jitter(data = top_err, aes(y = 0, color = id,
text = paste0("id: ", id,
"\n", key, ":", round(value,2),
"\nerror: ", round(pred_error,2))),
alpha = 0.5, width = 0.01, height = 50) +
scale_color_manual(values = color_vec) +
facet_wrap(~ key, scales = "free_x") +
theme(panel.spacing.y = unit(2, "lines")),
tooltip = c('text'))
htmlwidgets::saveWidget(pred_erly, file = "pred_erly.html", selfcontained = TRUE)
set.seed(1234)
kfold <- 3
rcpKFold <- recipe(target_var ~ ., data = listing_train) %>%
step_naomit(all_predictors(), -all_nominal(), skip = TRUE) #remove rows with NA
listing_kfolds <- rcpKFold %>%
prep() %>%
juice() %>%
vfold_cv(v = kfold, strata = target_var)
#GLM model
set.seed(1234)
glmnet_model <- linear_reg(mode = "regression",
penalty = tune(),
mixture = tune()) %>%
set_engine("glmnet")
#set tune specification
glmnet_params <- parameters(penalty(), mixture())
glmnet_grid <- grid_max_entropy(glmnet_params, size = 20)
#workflow
glm_wf <- workflow() %>%
add_model(glmnet_model) %>%
add_recipe(rcpTransform1)
glm_result <- glm_wf %>%
tune_grid(resamples = listing_kfolds,
grid = glmnet_grid,
metrics = metric_set(mae, mape, rmse, rsq))
glm_p <- glm_result %>%
collect_metrics() %>%
ggplot(aes(penalty, mean, color = .metric)) +
geom_errorbar(aes(ymin = mean - std_err,
ymax = mean + std_err),
alpha = 0.5) +
geom_line(size = 1.5) +
facet_wrap(~.metric, scales = "free", nrow = 2) +
scale_x_log10() +
theme(legend.position = "none")
ggplotly(glm_p)
glm_result %>%
show_best("rmse")
best_glm_model <- glm_result %>%
select_best("rmse")
#finalise glm model
final_glm_wf <-
glm_wf %>%
finalize_workflow(best_glm_model)
final_glm <- final_glm_wf %>%
fit(data = listing_train)
glm_p <- final_glm %>%
pull_workflow_fit() %>%
tidy() %>%
filter(estimate!=0) %>%
arrange(estimate) %>%
ggplot(aes(x = estimate, y = reorder(term, estimate),
text = paste0(term, "\nestimate: ", round(estimate,3)))) +
geom_point() +
geom_vline(xintercept = 0, linetype = "dashed") +
theme(axis.title.y = element_blank()) +
ggtitle("GLM model")
glm_ply <- ggplotly(glm_p, tooltip = c('text'))
widgetframe::frameWidget(glm_ply)
final_fit_glm <- last_fit(final_glm_wf,
split = listing_split,
metrics = metric_set(mae, mape, rmse, rsq))
pred_p <- final_fit_glm %>%
collect_predictions() %>%
ggplot(aes(target_var, .pred)) +
geom_point() +
geom_abline() +
coord_obs_pred() +
labs(title = "R-squared plot",
x = paste0("Actual ", targetVar),
y = paste0("Predicted ", targetVar))
pred_ply2 <- ggplotly(pred_p)
widgetframe::frameWidget(pred_ply2)
final_metric_glm <- collect_metrics(final_fit_glm)
final_metric_glm
set.seed(1234)
tree_model <- decision_tree(cost_complexity = tune(),
tree_depth = tune(),
min_n = tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
tree_grid <- grid_regular(cost_complexity(),
tree_depth(range = c(3,7)),
min_n(),
levels = 3)
tree_wf <- workflow() %>%
add_model(tree_model) %>%
add_recipe(rcpTransform2)
tree_result <- tree_wf %>%
tune_grid(resamples = listing_kfolds,
grid = tree_grid,
metrics = metric_set(mae, mape, rmse, rsq),
control = control_resamples(save_pred = TRUE))
collect_metrics(tree_result)
autoplot(tree_result) + theme_light()
best_tree <- tree_result %>%
select_best("rmse")
tree_result %>%
show_best("rmse")
final_tree_wf <- tree_wf %>%
finalize_workflow(best_tree)
final_tree <- final_tree_wf %>%
fit(data = listing_train)
final_tree %>%
pull_workflow_fit() %>%
vip()
final_tree_fit <- final_tree %>%
pull_workflow_fit()
fancyRpartPlot(final_tree_fit$fit, sub = "Decision Tree model")
viz_tree <- visNetwork::visTree(final_tree_fit$fit)
widgetframe::frameWidget(viz_tree)
final_fit_tree <- last_fit(final_tree_wf,
split = listing_split,
metrics = metric_set(mae, mape, rmse, rsq))
final_fit_tree %>%
collect_predictions() %>%
ggplot(aes(target_var, .pred)) +
geom_point() +
geom_abline() +
coord_obs_pred() +
labs(title = "R-squared plot",
x = paste0("Actual ", targetVar),
y = paste0("Predicted ", targetVar))
multi_metric <- metric_set(mae, mape, rmse, rsq)
final_metric_dtree <- collect_metrics(final_fit_tree)
final_metric_dtree
tic("RF timing")
set.seed(1234)
#Random forest
randomf_model <-rand_forest(trees = 500,
mtry = tune(),
min_n = tune()) %>%
set_engine("ranger",
importance = "permutation") %>%
set_mode("regression")
randomf_wf <- workflow() %>%
add_model(randomf_model) %>%
add_recipe(rcpTransform3)
randomf_grid <- grid_regular(mtry(range = c(10, 20)),
min_n(),
levels = 3)
randomf_result <- randomf_wf %>%
tune_grid(resamples = listing_kfolds,
grid = randomf_grid,
control = control_resamples(save_pred = TRUE),
metrics = metric_set(mae, mape, rmse, rsq))
collect_metrics(randomf_result)
autoplot(randomf_result) + theme_light()
best_rf <- randomf_result %>%
select_best("rmse")
randomf_result %>%
show_best("rmse")
final_rf_wf <- randomf_wf %>%
finalize_workflow(best_rf)
final_rf <- final_rf_wf %>%
fit(data = listing_train)
final_rf %>%
pull_workflow_fit() %>%
vip()
listing_test_T <- rcpTransform3 %>% prep() %>% bake(new_data = listing_test)
final_fit_rf <- predict(final_rf, new_data = listing_test) %>%
bind_cols(listing_test_T)
final_fit_rf %>%
# collect_predictions() %>%
ggplot(aes(target_var, .pred)) +
geom_point() +
geom_abline() +
coord_obs_pred() +
labs(title = "R-squared plot",
x = paste0("Actual ", targetVar),
y = paste0("Predicted ", targetVar))
final_metric_rf <- final_fit_rf %>%
multi_metric(truth = target_var, estimate = .pred)
toc()
tic("xgboost timing")
set.seed(1234)
xgboost_model <- boost_tree(mode = "regression",
trees = tune(),
min_n = tune(),
tree_depth = tune(),
learn_rate = tune()) %>%
set_engine("xgboost", objective = "reg:squarederror")
xgboost_params <- parameters(trees(), min_n(), tree_depth(), learn_rate())
xgboost_grid <- grid_max_entropy(xgboost_params, size = 20)
xgboost_wf <- workflow() %>%
add_model(xgboost_model) %>%
add_recipe(rcpTransform1)
xgboost_result <- xgboost_wf %>%
tune_grid(resamples = listing_kfolds,
grid = xgboost_grid,
metrics = metric_set(mae, mape, rmse, rsq))
xgb_p <- xgboost_result %>%
collect_metrics() %>%
mutate(tree_depth = factor(tree_depth)) %>%
ggplot(aes(learn_rate, mean, color = tree_depth)) +
geom_line(size = 1, alpha = 0.6) +
geom_point(size = 1.5) +
facet_wrap(~ .metric, scales = "free", nrow = 2) +
scale_x_log10(labels = scales::label_number()) +
theme_light()
xgb_ply <- ggplotly(xgb_p)
htmlwidgets::saveWidget(xgb_ply, file = "xgb_ply.html", selfcontained = TRUE)
metric_p <- final_metric_lm %>%
mutate(model = "LM") %>%
bind_rows(final_metric_glm %>% mutate(model = "GLM")) %>%
bind_rows(final_metric_dtree %>% mutate(model = "DTree")) %>%
bind_rows(final_metric_rf %>% mutate(model = "RdmForest")) %>%
bind_rows(final_metric_xgb %>% mutate(model = "XGBoost")) %>%
ggplot(aes(reorder_within(model, -.estimate, .metric), .estimate,
text = paste0("Model: ", model, "\n",
.metric, ": ", round(.estimate, 3)))) +
geom_bar(stat = 'identity') +
scale_x_reordered() +
facet_wrap(~ .metric, scales = "free") +
ylab("Estimate") +
theme(axis.title.x = element_blank(),
panel.spacing.y = unit(2, "lines"))
xgboost_result %>%
show_best("rmse")
best_xgboost_model <- xgboost_result %>%
select_best("rmse")
final_xgboost_wf <-
xgboost_wf %>%
finalize_workflow(best_xgboost_model)
final_xgboost <- final_xgboost_wf %>%
fit(data = listing_train)
final_xgboost <- final_xgboost_wf %>%
fit(data = listing_train)
final_xgboost %>%
pull_workflow_fit() %>%
vip()
xgboost_result %>%
show_best("rmse")
best_xgboost_model <- xgboost_result %>%
select_best("rmse")
final_xgboost_wf <-
xgboost_wf %>%
finalize_workflow(best_xgboost_model)
final_xgboost <- final_xgboost_wf %>%
fit(data = listing_train)
final_xgboost %>%
pull_workflow_fit() %>%
vip()
final_fit_xgboost <- last_fit(final_xgboost_wf,
split = listing_split,
metrics = metric_set(mae, mape, rmse, rsq))
final_fit_xgboost %>%
collect_predictions() %>%
ggplot(aes(target_var, .pred)) +
geom_point() +
geom_abline() +
coord_obs_pred() +
labs(title = "R-squared plot",
x = paste0("Actual ", targetVar),
y = paste0("Predicted ", targetVar))
final_metric_xgb <- collect_metrics(final_fit_xgboost)
toc()
metric_p <- final_metric_lm %>%
mutate(model = "LM") %>%
bind_rows(final_metric_glm %>% mutate(model = "GLM")) %>%
bind_rows(final_metric_dtree %>% mutate(model = "DTree")) %>%
bind_rows(final_metric_rf %>% mutate(model = "RdmForest")) %>%
bind_rows(final_metric_xgb %>% mutate(model = "XGBoost")) %>%
ggplot(aes(reorder_within(model, -.estimate, .metric), .estimate,
text = paste0("Model: ", model, "\n",
.metric, ": ", round(.estimate, 3)))) +
geom_bar(stat = 'identity') +
scale_x_reordered() +
facet_wrap(~ .metric, scales = "free") +
ylab("Estimate") +
theme(axis.title.x = element_blank(),
panel.spacing.y = unit(2, "lines"))
metric_ply <- ggplotly(metric_p, tooltip = c("text"))
htmlwidgets::saveWidget(metric_ply, file = "metric_ply.html", selfcontained = TRUE)
Blogdown::build_site(build_rmd = 'timestamp')
blogdown::build_site(build_rmd = 'timestamp')
blogdown::hugo_build(args = "--noTimes")
