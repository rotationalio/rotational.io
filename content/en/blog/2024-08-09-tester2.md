---
title: "Tester2"
slug: "tester2"
date: "2024-08-07T21:01:57-04:00"
draft: false
image: img/blog/
photo_credit: "Add Photo Credits Here"
authors: ['Nneka Okigbo']
profile: img/team/nneka-okigbo.png
tags: ['Add Tag 1 Here', 'Add Tag 2 Here']
description: "Add Description Here"
---

Add description or snippet here.

<!--more-->
I think it's fitting how in the engineering world, a 

## What is SBIR?

[SBIR (Small Business Innovation Research)](https://www.sbir.gov) is a program that allows agencies within the government to provide funding to small businesses. SBIR encourages small businesses to participate in government research and development in hopes of a product being commericialized.

SBIR has several topic areas that businesses can submit proposals in order to receive an opportunity to work with an agency. However, the embedded search engine in on the SBIR website is ineffecient (maybe say why it was ineffcient). Rebecca Bilbro noticed this and proposed my internship project idea.

I would be tasked with building a recommender system that efficiently outputed agency topic areas that modeled Rotational Lab's mission and values.

## Approach 1: Semantic Similarity

Luckily, retrieving the data on the agency topic areas was a breeze. Although the website has changed now, at the time, the [SBIR website](https://www.sbir.gov/topics) allowed for easy data collection by providing the data in json form.

With the help of Patrick Deziel, I decided my first approach would be a [semantic similarity model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) I retrieved from Hugging Face.

The semantic similarity approach seemed ideal since we already had topic areas that we had approved. Our two approved topic areas were titled [Context Aware Data Stream Pre-processor for Time-Sensitive Applications](https://www.sbir.gov/topics/10835) and [Signal Cueing in Complex Environments](https://www.sbir.gov/topics/10837). My idea was the compare other topics areas with one's we had already approved. 

```python
def similarity(text, df, n=3):
   embeddings = model1.encode(text)
   df["Similarity_Score"] = df["Encoded_Text"].apply(lambda x: util.cos_sim(embeddings, x).item())
   return df.sort_values(by="Similarity_Score", ascending=False).head(15)

text = sbir_data["Description"].iloc[ideal_ta2]
similarity(text, sbir_data, n = 10)[["TopicTitle", "Similarity_Score", "CloseDate"]]
```

The topic areas that had higher scores were deemed more semantically similar to the ones we had chosen, therefore being good candidates. I ranked the top 10 similarity scores from highest to lowest and printed the output for both both approved topic areas.

!["Semantic Similarity 1"](/img/blog/2024-08-09-my-tester-blog/first-semanticsim.png)
!["Semantic Similarity 2"](/img/blog/2024-08-09-my-tester-blog/second-semanticsim.png)

As I received into the outputted topic areas, I dug deeper into the full descriptions of them to see whether they truly match Rotational Lab's mission and values. The results were very hit or miss. Some of the topic areas were very far off while the others required a different expertise than Rotational provides. There were few topics that were ideal.

### Approach 1b: Semantic Similarity on Summarized Text
Another approach that I thought would be reasonable would be a semantic similarity approach on summarized text. To do this I would select a [summarization model](https://huggingface.co/facebook/bart-large-cnn) from Hugging Face. I thought this might be a better idea since the similarity model could grasp the idea of the text then compare the similarity of the ideas.

Unfortunately, this approach proved to be even less accurate than the former. All of the inaccuracies resulted from the semantic similarity approach could have been for several reasons.

1. We were limited to comparing the descriptions to pre-approved topic areas.

2. We could not properly find topic areas that adhered to Rotational's mission because we were limited to what we could compare it to.

3. The summarization model did not allow enough tokens to properly summarize the descriptions of the topic areas, resulting in larges chunks of text to be cut out the the summarization.

4. Simply the model I chose for the semantic similarity approach was not ideal for the data I was working with.

Despite, the several flaws associated with this approach, the truth remained that I needed a more accurate approach.

## Approach 2: GLiNER 

GLiNER (Generalist Model for Named Entity Recognition using Bidirectional Transformer) is a Named Entity Recognition (NER) model that is a practical alternative to other NER options. Unlike other options, GLiNER is not restricted to predefined entities. Usually other NER models have limited to simple labels such as person, place, organization, etc. GLiNER allows users to select any label they desire.

The idea for this approach was to select labels that adhered to Rotational's mission and values. For this I went on [Rotational's About page](https://rotational.io/about/)

```python
labels = ["data", "machine learning", "native computing", "event driven", "data systems", "artificial intelligence"]
```
Since, we are grabbing words and phrases directly from Rotational Labs, we can better eliminate topic areas that have nothing to do with our values. Woohoo to solving problem #2!

So I created a function that takes 3 inputs: a pandas dataframe, the selected column of text, and the labels. I also created new columns that show the phrase of text that was associated with the labels.

```python
def top10_withGLiNER(df, column, labels):
 
    model = GLiNER.from_pretrained("urchade/gliner_medium-v2.1")

    labels = labels
    def predict_entities(text):
        entities = model.predict_entities(text, labels)
        return [(entity["text"], entity["label"]) for entity in entities]

    df['entities'] = df[column].apply(predict_entities)

    # Counting number of entities found
    df['entity_count'] = df['entities'].apply(len)

    df['entity_texts'] = df['entities'].apply(lambda x: [entity[0] for entity in x])
    df['entity_labels'] = df['entities'].apply(lambda x: [entity[1] for entity in x])

    return df.sort_values(by="entity_count", ascending=False).head(10)

top10_withGLiNER(sbir_data, 'Description2', labels)[["TopicTitle", "entity_count", "CloseDate", "entities"]]
```
!["GLiNER Results"](/img/blog/2024-08-09-my-tester-blog/gliner-results.png)

say something about results
say something about the effectiveness of this approach
say something about its flaws (rotationals values may not translate to government wurds)


## Approach 3: Annotating and Training with Prodigy

Although using the GLiNER approach was interesting and enjoyable, it was clear that there were better approaches. With the helpful advice of 




hello blah blah