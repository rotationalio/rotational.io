<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLMs on Rotational Labs</title><link>https://rotational.io/tags/llms/</link><description>Recent content in LLMs on Rotational Labs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Sep 2024 14:25:57 -0400</lastBuildDate><atom:link href="https://rotational.io/tags/llms/index.xml" rel="self" type="application/rss+xml"/><item><title>Recapping PyTorch: Key Takeaways from the 2024 Conference</title><link>https://rotational.io/blog/pytorch-conference-2024/</link><pubDate>Tue, 24 Sep 2024 14:25:57 -0400</pubDate><guid>https://rotational.io/blog/pytorch-conference-2024/</guid><description>&lt;p>I spent last week in San Francisco meeting up with the Rotational team to attend &lt;a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference&lt;/a>. If you&amp;rsquo;re an LLM developer and didn&amp;rsquo;t make it this year, here are some of my key highlights and takeaways.&lt;/p></description></item><item><title>Teaching LLMs With Continuous Human Feedback</title><link>https://rotational.io/blog/teaching-llms-with-human-feedback/</link><pubDate>Fri, 13 Sep 2024 13:38:26 -0500</pubDate><guid>https://rotational.io/blog/teaching-llms-with-human-feedback/</guid><description>&lt;p>If you&amp;rsquo;ve worked with generative AI models you know they can be fickle and sometimes fail to meet the expectations of users. How can we move towards models users trust and see clear value in? Let&amp;rsquo;s engineer a user-feedback loop!&lt;/p></description></item><item><title>To LLM or Not to LLM (Part 2): Starting Simple</title><link>https://rotational.io/blog/starting-simple-with-ai/</link><pubDate>Mon, 20 May 2024 09:00:00 -0500</pubDate><guid>https://rotational.io/blog/starting-simple-with-ai/</guid><description>&lt;p>Sick of hearing about hyped up AI solutions that sound like hot air? 🧐 Let&amp;rsquo;s use boring old ML to detect hype in AI marketing text and see why starting with a simple ML approach is still your best bet 90% of the time.&lt;/p></description></item><item><title>To LLM or Not to LLM: Tips for Responsible Innovation</title><link>https://rotational.io/blog/responsible-innovation/</link><pubDate>Wed, 08 May 2024 09:33:34 -0400</pubDate><guid>https://rotational.io/blog/responsible-innovation/</guid><description>&lt;p>We&amp;rsquo;re seeing a proliferation of Large Language Models (LLMs) as companies seek to replicate OpenAI&amp;rsquo;s success. In this post, two AI engineers respond to LLM FAQs and offer tips for responsible innovation.&lt;/p></description></item><item><title>Predicting the Oscars With LLMs</title><link>https://rotational.io/blog/predicting-the-oscars-with-llms/</link><pubDate>Fri, 08 Mar 2024 15:17:58 -0600</pubDate><guid>https://rotational.io/blog/predicting-the-oscars-with-llms/</guid><description>&lt;p>Looking for a middle ground between custom LLMs and traditional ML? Please welcome semantic search to the stage! Let&amp;rsquo;s use semantic search to predict which film will take home the &amp;ldquo;Best Picture&amp;rdquo; Oscar this year 🤩&lt;/p></description></item></channel></rss>