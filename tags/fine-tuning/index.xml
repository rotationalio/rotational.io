<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fine-tuning on Rotational Labs</title><link>https://rotational.io/tags/fine-tuning/</link><description>Recent content in Fine-tuning on Rotational Labs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Fri, 13 Sep 2024 13:38:26 -0500</lastBuildDate><atom:link href="https://rotational.io/tags/fine-tuning/index.xml" rel="self" type="application/rss+xml"/><item><title>Teaching LLMs With Continuous Human Feedback</title><link>https://rotational.io/blog/teaching-llms-with-human-feedback/</link><pubDate>Fri, 13 Sep 2024 13:38:26 -0500</pubDate><guid>https://rotational.io/blog/teaching-llms-with-human-feedback/</guid><description>&lt;p>If you&amp;rsquo;ve worked with generative AI models you know they can be fickle and sometimes fail to meet the expectations of users. How can we move towards models users trust and see clear value in? Let&amp;rsquo;s engineer a user-feedback loop!&lt;/p></description></item></channel></rss>