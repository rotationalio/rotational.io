<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fine-Tuning on Rotational Labs</title><link>https://rotational.io/tags/fine-tuning/</link><description>Recent content in Fine-Tuning on Rotational Labs</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 24 Sep 2024 14:25:57 -0400</lastBuildDate><atom:link href="https://rotational.io/tags/fine-tuning/index.xml" rel="self" type="application/rss+xml"/><item><title>Recapping PyTorch: Key Takeaways from the 2024 Conference</title><link>https://rotational.io/blog/pytorch-conference-2024/</link><pubDate>Tue, 24 Sep 2024 14:25:57 -0400</pubDate><guid>https://rotational.io/blog/pytorch-conference-2024/</guid><description>&lt;p>I spent last week in San Francisco meeting up with the Rotational team to attend &lt;a href="https://events.linuxfoundation.org/pytorch-conference/">PyTorch Conference&lt;/a>. If you&amp;rsquo;re an LLM developer and didn&amp;rsquo;t make it this year, here are some of my key highlights and takeaways.&lt;/p></description></item><item><title>Teaching LLMs With Continuous Human Feedback</title><link>https://rotational.io/blog/teaching-llms-with-human-feedback/</link><pubDate>Fri, 13 Sep 2024 13:38:26 -0500</pubDate><guid>https://rotational.io/blog/teaching-llms-with-human-feedback/</guid><description>&lt;p>If you&amp;rsquo;ve worked with generative AI models you know they can be fickle and sometimes fail to meet the expectations of users. How can we move towards models users trust and see clear value in? Let&amp;rsquo;s engineer a user-feedback loop!&lt;/p></description></item></channel></rss>